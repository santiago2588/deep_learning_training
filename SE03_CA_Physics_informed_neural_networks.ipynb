{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/se_03.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/SE03_CA_Physics_informed_neural_networks.ipynb)\n",
    "# Workshop Instructions\n",
    "***\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> Follow along by typing the code yourself - this helps with learning!\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> Code cells marked as \"Exercise\" are for you to complete\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> Look for hints if you get stuck\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/success.svg\" width=\"20\" /> Compare your solution with the provided answers\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/list.svg\" width=\"20\" /> Don't worry if you make mistakes - debugging is part of learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     0K .                                                     100% 5.27M=0s\n",
      "\n",
      "\n",
      "     0K .                                                     100%  522K=0.003s\n",
      "\n",
      "\n",
      "     0K                                                       100% 6.27M=0s\n",
      "\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 90% 2.93M 0s\n",
      "    50K .....                                                 100% 60.1M=0.02s\n",
      "\n",
      "\n",
      "     0K .......... ........                                   100% 44.0M=0s\n",
      "\n",
      "\n",
      "     0K .......... .......... .......... ..........           100% 6.30M=0.006s\n",
      "\n",
      "\n",
      "     0K                                                       100% 19.6M=0s\n",
      "\n",
      "\n",
      "     0K .......                                               100% 2.07M=0.004s\n",
      "\n",
      "\n",
      "     0K ..........                                            100%  103M=0s\n",
      "\n",
      "\n",
      "     0K .......... .                                          100% 79.5M=0s\n",
      "\n",
      "\n",
      "     0K                                                       100% 11.6M=0s\n",
      "\n",
      "\n",
      "     0K ...                                                   100% 75.8M=0s\n",
      "\n",
      "\n",
      "     0K ....                                                  100% 3.11M=0.001s\n",
      "\n",
      "\n",
      "     0K ........                                              100% 1.71M=0.005s\n",
      "\n",
      "\n",
      "     0K .                                                     100% 46.8M=0s\n",
      "\n",
      "\n",
      "     0K ..                                                    100% 2.62M=0.001s\n",
      "\n",
      "\n",
      "     0K .......... ..                                         100% 4.27M=0.003s\n",
      "\n",
      "\n",
      "     0K .......... ......                                     100% 4.59M=0.003s\n",
      "\n",
      "\n",
      "     0K .......... .......... ..                              100%  122M=0s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt -O colab_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=3 -i colab_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faculty of Science and Engineering ðŸ”¬\n",
      "\u001b[95mThe University of Manchester \u001b[0m\n",
      "Invoking utils version: \u001b[92m1.1.0+1301734\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_path = Path.cwd()\n",
    "if str(repo_path) not in sys.path:\n",
    "    sys.path.append(str(repo_path))\n",
    "\n",
    "import utils\n",
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "helper_utils = Path.cwd().parent\n",
    "sys.path.append(str(helper_utils))\n",
    "\n",
    "# Initialize the exercise checker for SE03\n",
    "checker = utils.core.ExerciseChecker(\"SE03P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to PINNs\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Physics-Informed Neural Networks (PINNs) are neural networks that are trained to solve supervised learning tasks while respecting physical laws described by partial differential equations.\n",
    "\n",
    "Physics-Informed Neural Networks (PINNs) are a class of deep learning models that integrate physical laws, typically expressed as partial differential equations (PDEs), directly into the learning process. Instead of relying solely on data, PINNs leverage known physics to constrain the model, allowing for more robust learning, especially in scenarios where data is scarce or noisy.\n",
    "\n",
    "PINNs combine two major concepts:\n",
    "\n",
    "| Component | Description | Role |\n",
    "|-----------|-------------|------|\n",
    "| **Neural Networks** | Deep learning models that can approximate complex functions | Learn patterns from data |\n",
    "| **Physical Laws** | Mathematical equations describing system behavior | Enforce physical constraints |\n",
    "\n",
    "## 1.1 Why PINNs?\n",
    "***\n",
    "Traditional numerical methods for solving PDEs face several challenges:\n",
    "\n",
    "| Challenge | Traditional Methods | PINN Solution |\n",
    "|-----------|---------------------|---------------|\n",
    "| **Computational Cost** | High for complex geometries | Efficient once trained |\n",
    "| **Mesh Requirements** | Need fine meshes | Meshless approach |\n",
    "| **Limited Data** | Require complete boundary conditions | Can work with sparse data |\n",
    "| **High Dimensions** | Suffer from curse of dimensionality | Better scaling with dimensions |\n",
    "| **Generalisation** | Limited to specific problems | Generalises to new conditions |\n",
    "\n",
    "\n",
    "# 2. Case Study: Navier-Stokes Equations\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, forming the basis of fluid dynamics.\n",
    "\n",
    "These equations govern the motion of incompressible fluids in 2D:\n",
    "\n",
    "**Momentum equations:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial u}{\\partial t} + \\lambda_1 (u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y}) &= -\\frac{\\partial p}{\\partial x} + \\lambda_2 (\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}) \\quad \\text{(momentum in x-direction)} \\\\\n",
    "\\frac{\\partial v}{\\partial t} + \\lambda_1 (u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y}) &= -\\frac{\\partial p}{\\partial y} + \\lambda_2 (\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}) \\quad \\text{(momentum in y-direction)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Continuity equation (incompressibility condition):**\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0\n",
    "$$\n",
    "\n",
    "**Meaning of terms:**\n",
    "- $u(x, y, t)$: horizontal velocity\n",
    "- $v(x, y, t)$: vertical velocity\n",
    "- $p(x, y, t)$: pressure\n",
    "- $\\lambda_1$: convection coefficient (usually 1)\n",
    "- $\\lambda_2 = \\nu $: kinematic viscosity\n",
    "\n",
    "These equations state that:\n",
    "- Fluids accelerate due to pressure differences and internal friction (viscosity)\n",
    "- Mass is conserved â†’ the flow remains incompressible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Flow past a cylinder\n",
    "***\n",
    "In this notebook we are going to explore a realistic scenario of incompressible fluid flow as described by the ubiquitous Navier-Stokes equations. Navier-Stokes equations describe the physics of many phenomena of scientific and engineering. Often, the Navier-Stokes equations are solved using numerical methods, such as finite element or finite volume methods. However, these methods can be computationally expensive and time-consuming, especially for complex geometries and boundary conditions.\n",
    "In this workshop, we will use a dataset of incompressible fluid flow around a cylinder. The dataset is generated using a finite volume method and contains the velocity and pressure fields of the fluid flow. The dataset consists of the following variables:\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| u        | x-component of velocity |\n",
    "| v        | y-component of velocity |\n",
    "| p        | pressure |\n",
    "| t        | time |\n",
    "\n",
    "The dataset was prepared using the following simulation parameters:\n",
    "- **Domain**: \\( [-15, 25] \\times [-8, 8] \\)\n",
    "- **Reynolds number**: \\( Re = 100 \\)\n",
    "- **Numerical method**: Spectral/hp-element solver (NekTar)\n",
    "- **Mesh**: 412 triangular elements, 10th-order basis functions\n",
    "- **Integration**: Third-order stiff scheme until steady vortex shedding\n",
    "\n",
    "\n",
    "For this problem, we want to predict the Convective term $\\lambda_1$, the viscous term $\\lambda_2$, as well as a reconstruction of the pressure field $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:\n",
      "Cylinder dataset for predicting the drag coefficient of a cylinder in a flow field\n",
      "> Authors: Maziar Raissi1, Paris Perdikaris, George Em Karniadakis\n",
      "> Year: 2017\n",
      "> Website: https://arxiv.org/pdf/1711.10566.pdf\n",
      "\n",
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('cylinder',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=False,\n",
    "                                   remove_compressed=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5000 points in space, 2 dimensions and 200 time steps.\n"
     ]
    }
   ],
   "source": [
    "data = scipy.io.loadmat(dataset_path)\n",
    "u_star = data['U_star'] # velocity n x 2 x time\n",
    "p_star = data['p_star'] # pressure n x time\n",
    "x_star = data['X_star'] # coordinates n x 2\n",
    "time = data['t'] # time n x 1\n",
    "\n",
    "print(f'We have {u_star.shape[0]} points in space, {u_star.shape[1]} dimensions and {u_star.shape[2]} time steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7075fb82f84531b007372b46c8090d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Time Step', max=199), Output()), _dom_classes=('widget-iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6896a272d64c4714b1b4b728ad82d03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plotting.wake_cylinder_interactive(x_star, u_star, p_star, time, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the dataset\n",
    "***\n",
    "As you can see in the image above, the simulation has been cropped to a smaller domain that does not include the cylinder. Thus, with this dataset we are only using 1% of the total data for training. This is to highlight the ability of PINNs to learn from limited data. In practice, you would typically use a larger portion of the dataset for training. Thus, we are not going to split the dataset into training, validation, and test sets. Instead, we will use the entire dataset for training and testing. This is a common practice in PINNs, where the model is trained on a small portion of the data and then tested on the entire dataset.\n",
    "\n",
    "The training data consists of the following parameters:\n",
    "- Region: Small rectangle downstream of cylinder\n",
    "- Sampled: \\( u(x, y, t), v(x, y, t) \\)\n",
    "- Size: 5,000 points (\\~1% of simulation)\n",
    "- **No pressure data used**\n",
    "\n",
    "To prepare the data we need to load the dataset and extract the relevant variables. Furthermore, we need to reshape the data to be compatible with the PINN model. \n",
    "\n",
    "## 3.1 Normalisation\n",
    "***\n",
    "For PINNs, the features cannot be normalised in the same way as in traditional machine learning. Standard normalization in PINNs is tricky because physical equations must remain consistent with the scaled variables. The normalisation should be applied to the loss function considering the physics of the problem. This is a more complex process and requires a deeper understanding of the physical equations involved. \n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/list.svg\" width=\"20\" /> In this workshop, we will not cover this topic in detail, but it is important to keep in mind that normalisation in PINNs is not as straightforward as in traditional machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "âœ… x_train is correct!\n",
      "âœ… y_train is correct!\n",
      "âœ… t_train is correct!\n",
      "âœ… u_train is correct!\n",
      "âœ… v_train is correct!\n",
      "âœ… data_shape is correct!\n",
      "\n",
      "ðŸŽ‰ Excellent! All parts are correct!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6: Prepare the data for training ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Create a meshgrid-like structure for the x, y, and t coordinates\n",
    "# 2. Flatten the velocity and pressure data into NT x 1 arrays\n",
    "# 3. Randomly sample N points from the flattened data\n",
    "# 4. Convert the sampled data into PyTorch tensors\n",
    "\n",
    "N, T = x_star.shape[0], time.shape[0] # number of points in space and time\n",
    "\n",
    "# Create coordinate grids \n",
    "x_flat = x_star[:, 0]  # Extract x coordinates\n",
    "y_flat = x_star[:, 1]  # Extract y coordinates\n",
    "t_flat = time.flatten() # Flatten time array\n",
    "\n",
    "# Create meshgrid-like structures for visualization\n",
    "x_coords = np.tile(x_flat[:, np.newaxis], (1, T))  # Repeat x for each timestep\n",
    "y_coords = np.tile(y_flat[:, np.newaxis], (1, T))  # Repeat y for each timestep\n",
    "time_coords = np.tile(t_flat, (N, 1))              # Repeat time for each spatial point\n",
    "\n",
    "# Extract velocity and pressure data\n",
    "u_vals = u_star[:, 0, :]  # Extract u velocity\n",
    "v_vals = u_star[:, 1, :]  # Extract v velocity\n",
    "p_vals = p_star[:, :]     # Extract pressure\n",
    "\n",
    "# Flatten into NT x 1 arrays\n",
    "x = x_coords.flatten()[:, np.newaxis]\n",
    "y = y_coords.flatten()[:, np.newaxis]\n",
    "t = time_coords.flatten()[:, np.newaxis]\n",
    "u = u_vals.flatten()[:, np.newaxis]\n",
    "v = v_vals.flatten()[:, np.newaxis]\n",
    "p = p_vals.flatten()[:, np.newaxis]\n",
    "\n",
    "idx = np.random.choice(N*T, N, replace=False)\n",
    "x_train = x[idx, :]\n",
    "y_train = y[idx, :]\n",
    "t_train = t[idx, :]\n",
    "u_train = u[idx, :]\n",
    "v_train = v[idx, :]\n",
    "\n",
    "# Preparing the data as tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "t_train = torch.tensor(t_train, dtype=torch.float32)\n",
    "u_train = torch.tensor(u_train, dtype=torch.float32)\n",
    "v_train = torch.tensor(v_train, dtype=torch.float32)\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    't_train': t_train,\n",
    "    'u_train': u_train,\n",
    "    'v_train': v_train,\n",
    "    'data_shape': x_train.shape[0]\n",
    "}\n",
    "checker.check_exercise(6, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PINN Architecture\n",
    "***\n",
    "In this workshop we are going to use a Physics-Informed Neural Network (PINN) to solve the Navier-Stokes equations. The PINN model is a neural network that is trained to satisfy the Navier-Stokes equations, as well as the boundary conditions of the problem. The PINN model consists of the following components:\n",
    "\n",
    "| Component | Purpose | Implementation |\n",
    "|-----------|---------|----------------|\n",
    "| **Input Layer** | Takes spatial/temporal coordinates | $(x, y, t)$ coordinates |\n",
    "| **Hidden Layers** | Learn the underlying patterns | Multiple fully connected layers |\n",
    "| **Output Layer** | Predicts physical quantities | Velocity and pressure fields |\n",
    "| **Physics Loss** | Enforces PDE constraints | Automatic differentiation |\n",
    "\n",
    "***\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/pinn.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "For the connected layers we are going to use the following activation functions:\n",
    "\n",
    "| Layer | Activation Function |\n",
    "|-------|---------------------|\n",
    "| Input Layer | Tanh |\n",
    "| Hidden Layers | Tanh |\n",
    "| Output Layer | Tanh |\n",
    "\n",
    "The choice of activation function for the output layer is important, as it can affect the range of the output values. In this case, we are using Tanh to ensure that the output values are in the range [-1, 1]. This is important for the PINN model, as we want to ensure that the predicted values are in the same range as the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "âœ… model is correct!\n",
      "âœ… hidden_size is correct!\n",
      "âœ… input_size is correct!\n",
      "âœ… output_size is correct!\n",
      "âœ… num_layers is correct!\n",
      "\n",
      "ðŸŽ‰ Excellent! All parts are correct!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7: Model Creation with Weight Initialization ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Define a class for the Navier-Stokes PINN model\n",
    "# 2. Initialize the model with a specified number of hidden layers and neurons\n",
    "# 3. Use Xavier initialization for the weights and biases of each layer\n",
    "# 4. Define the forward pass to compute the velocity and pressure outputs\n",
    "# 5. Split the output into u, v, and p components \n",
    "\n",
    "class NavierStokesPINN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=20, num_layers=9):\n",
    "        super().__init__()\n",
    "        self.nu = 0.01  # kinematic viscosity\n",
    "        \n",
    "        # Neural network architecture\n",
    "        layers = []\n",
    "        # Input layer: x, y, t\n",
    "        input_layer = torch.nn.Linear(3, hidden_size)\n",
    "        torch.nn.init.xavier_normal_(input_layer.weight)\n",
    "        torch.nn.init.zeros_(input_layer.bias)\n",
    "        \n",
    "        layers.append(input_layer)\n",
    "        \n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            hidden_layer = torch.nn.Linear(hidden_size, hidden_size)\n",
    "            torch.nn.init.xavier_normal_(hidden_layer.weight)\n",
    "            torch.nn.init.zeros_(hidden_layer.bias)\n",
    "            \n",
    "            layers.append(hidden_layer)\n",
    "            \n",
    "        # Output layer:  u,v,p because we are enforcing the continuity equation\n",
    "        output_layer = torch.nn.Linear(hidden_size, 3)\n",
    "        torch.nn.init.xavier_normal_(output_layer.weight)\n",
    "        torch.nn.init.zeros_(output_layer.bias)\n",
    "        layers.append(output_layer)\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        # Combine inputs\n",
    "        xyz = torch.cat([x, y, t], dim=1)\n",
    "        \n",
    "        # Forward pass through network\n",
    "        for i in range(len(self.net)-1):\n",
    "            xyz = torch.tanh(self.net[i](xyz))\n",
    "        \n",
    "        # Final layer without activation\n",
    "        output = self.net[-1](xyz)\n",
    "        \n",
    "        # Split output into u, v, p\n",
    "        u = output[:, 0:1]\n",
    "        v = output[:, 1:2]\n",
    "        p = output[:, 2:3]\n",
    "        \n",
    "        return u, v, p\n",
    "\n",
    "model = NavierStokesPINN(hidden_size=20, num_layers=9)\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'model': model,\n",
    "    'hidden_size': model.net[0].out_features,\n",
    "    'input_size': model.net[0].in_features,\n",
    "    'output_size': model.net[-1].out_features,\n",
    "    'num_layers': len(model.net)\n",
    "}\n",
    "checker.check_exercise(7, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. PINN Loss Function\n",
    "***\n",
    "When working with PINNs, the loss function is a crucial component that combines data loss and physics loss. The data loss measures how well the model fits the training data, while the physics loss measures how well the model satisfies the physical constraints defined by the PDEs. Thus, the loss function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{Data Loss} + \\text{Physics Loss}$$\n",
    "\n",
    "$$\\text{Data Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (u_i - u_{\\text{pred}, i})^2 + (v_i - v_{\\text{pred}, i})^2$$\n",
    "$$\\text{Physics Loss} = \\frac{1}{M} \\sum_{j=1}^{M} (f_{u,j}^2 + f_{v,j}^2 + f_{c,j}^2)$$\n",
    "\n",
    "Where:\n",
    "- $f_u = \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} + \\frac{\\partial p}{\\partial x} - \\nu \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)$\n",
    "- $f_v = \\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} + \\frac{\\partial p}{\\partial y} - \\nu \\left( \\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2} \\right)$\n",
    "- $f_c = \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y}$\n",
    "- $f_c$: Continuity equation (incompressibility condition)\n",
    "- $f_u$: Momentum equation in x-direction\n",
    "- $f_v$: Momentum equation in y-direction\n",
    "- $N$: Number of data points in the training set\n",
    "- $M$: Number of points in the physics loss (collocation points)\n",
    "\n",
    "The physics loss ensures that the solution satisfies the Navier-Stokes equations, while the data loss ensures the solution matches known data points. This dual optimization approach is what makes PINNs powerful for solving PDEs.\n",
    "\n",
    "## 5.1 Automatic Differentiation in PINNs\n",
    "***\n",
    "One of the key features of PINNs is their ability to automatically compute derivatives using automatic differentiation (autograd). This is crucial for enforcing physical constraints defined by PDEs.\n",
    "\n",
    "### 5.1.1 How Automatic Differentiation Works\n",
    "When we use `torch.autograd.grad()`, PyTorch computes derivatives through the computational graph:\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: Basic usage of `torch.autograd.grad()`\n",
    "\n",
    "```python\n",
    "u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "```\n",
    "\n",
    "The `torch.autograd.grad()` function returns a tuple of gradients. The first element is the gradient of `u` with respect to `t`. \n",
    "\n",
    "| Parameter | Purpose | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `u` | Target tensor | The output we want to differentiate |\n",
    "| `t` | Source tensor | The variable we're differentiating with respect to |\n",
    "| `grad_outputs` | Scaling factor | Usually ones, for direct gradient computation |\n",
    "| `create_graph` | Enable higher derivatives | Needed for second derivatives |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "âœ… f_u_shape is correct!\n",
      "âœ… f_v_shape is correct!\n",
      "âœ… f_c_shape is correct!\n",
      "âœ… has_gradients is correct!\n",
      "\n",
      "ðŸŽ‰ Excellent! All parts are correct!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 8: Compute Residuals ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Define a function to compute the residuals of the Navier-Stokes equations\n",
    "# 2. Enable gradients for the input variables (x, y, t)\n",
    "# 3. Compute the first and second derivatives of the velocity and pressure fields\n",
    "# 4. Calculate the residuals for the u and v momentum equations and the continuity equation\n",
    "# 5. Return the residuals as outputs\n",
    "\n",
    "def compute_ns_residuals(model, x, y, t):\n",
    "    # Enable gradients\n",
    "    x.requires_grad_(True)\n",
    "    y.requires_grad_(True) \n",
    "    t.requires_grad_(True)\n",
    "\n",
    "    # Get predictions\n",
    "    u, v, p = model(x, y, t)\n",
    "\n",
    "    # First derivatives\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    \n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    \n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    # Second derivatives\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    \n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "\n",
    "    # Compute residuals\n",
    "    f_u = u_t + (u * u_x + v * u_y) + p_x - model.nu * (u_xx + u_yy)\n",
    "    f_v = v_t + (u * v_x + v * v_y) + p_y - model.nu * (v_xx + v_yy)\n",
    "    f_c = u_x + v_y  # continuity equation\n",
    "\n",
    "    return f_u, f_v, f_c\n",
    "\n",
    "# Create a small test case to verify the residual calculation\n",
    "test_x = torch.ones((5, 1), requires_grad=True)\n",
    "test_y = torch.ones((5, 1), requires_grad=True)\n",
    "test_t = torch.ones((5, 1), requires_grad=True)\n",
    "\n",
    "test_fu, test_fv, test_fc = compute_ns_residuals(model, test_x, test_y, test_t)\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'f_u_shape': test_fu.shape,\n",
    "    'f_v_shape': test_fv.shape,\n",
    "    'f_c_shape': test_fc.shape,\n",
    "    'has_gradients': test_fu.requires_grad\n",
    "}\n",
    "checker.check_exercise(8, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PINN Optimiser and training\n",
    "***\n",
    "When training a PINN model, the choice of optimiser is crucial for achieving good performance. A common practice is to use a two-step approach: \n",
    "\n",
    "1. Use a standard optimiser (like Adam) to train the model, often called the \"warm-up\" phase. \n",
    "2. Switch to a more advanced optimiser (like L-BFGS) for fine-tuning.\n",
    "\n",
    "This approach allows for faster convergence in the initial phase and better performance in the final phase. \n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: The use of only Adam or L-BFGS is also possible, however, without the fine-tuning step the model will have an issue finding the right scale of the loss function. Resulting in a model that creates good qualitative results, but poor quantitative results.\n",
    "\n",
    "The warm-up phase follows the standard training process, where the model is trained using a standard optimiser (like Adam) for a certain number of epochs. The fine-tuning phase uses a more advanced optimiser (like L-BFGS) to refine the model parameters and improve performance.\n",
    "\n",
    "To use the L-BFGS optimiser, we need to define a closure function that computes the loss and gradients. The closure function is called by the optimiser to compute the loss and gradients, and it should return the loss value. The closure function should also zero out the gradients before computing the loss, as shown in the code snippet below.\n",
    "\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 2**: Using L-BFGS Optimiser\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "model = PINNModel()\n",
    "# Define the loss function\n",
    "loss_function = PINNLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.01)\n",
    "# Define the closure function\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(model)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.step(closure)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with Adam: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.31it/s, loss=1.0760]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "âœ… function_code is correct!\n",
      "âœ… has_adam is correct!\n",
      "âœ… has_lbfgs is correct!\n",
      "âœ… uses_closure is correct!\n",
      "âœ… has_data_loss is correct!\n",
      "âœ… has_physics_loss is correct!\n",
      "âœ… computes_residuals is correct!\n",
      "âœ… updates_weights is correct!\n",
      "âœ… uses_backpropagation is correct!\n",
      "âœ… learning_rate is correct!\n",
      "âœ… optimizer_params is correct!\n",
      "\n",
      "ðŸŽ‰ Excellent! All parts are correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 9: The training loop ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Define a training function for the PINN model\n",
    "# 2. Use Adam optimizer for the first phase of training\n",
    "# 3. Use L-BFGS optimizer for the second phase of training\n",
    "# 4. Compute the loss as a combination of data loss and physics loss\n",
    "# 5. Use a closure function to compute the loss and gradients for L-BFGS\n",
    "\n",
    "def train_pinn(model, data, epochs=10000, use_lbfgs=True):\n",
    "    x, y, t, u_true, v_true = data\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Move data to device\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    t = t.to(device)\n",
    "    u_true = u_true.to(device)\n",
    "    v_true = v_true.to(device)\n",
    "\n",
    "    # Adam optimization first\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Data loss\n",
    "        u_pred, v_pred, _ = model(x, y, t)\n",
    "        data_loss = mse_loss(u_pred, u_true) + mse_loss(v_pred, v_true)\n",
    "        \n",
    "        # Physics loss\n",
    "        f_u, f_v, f_c = compute_ns_residuals(model, x, y, t)\n",
    "        physics_loss = (torch.mean(f_u**2) + torch.mean(f_v**2) + \n",
    "                       torch.mean(f_c**2))\n",
    "        \n",
    "        # Total loss\n",
    "        loss = data_loss + physics_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # Train with Adam\n",
    "    pbar = tqdm(range(epochs), desc=\"Training with Adam\")\n",
    "    for _ in pbar:\n",
    "        loss = closure()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    # L-BFGS optimization\n",
    "    if use_lbfgs:\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), \n",
    "                                    max_iter=500,\n",
    "                                    max_eval=500,\n",
    "                                    tolerance_grad=1e-7,\n",
    "                                    tolerance_change=1e-7,\n",
    "                                    history_size=50,\n",
    "                                    line_search_fn=\"strong_wolfe\")\n",
    "        \n",
    "        print(\"Training with L-BFGS...\")\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Test with a smaller number of epochs\n",
    "test_model = NavierStokesPINN(hidden_size=20, num_layers=9)\n",
    "data_tuple = (x_train[:10], y_train[:10], t_train[:10], u_train[:10], v_train[:10])\n",
    "test_trained_model = train_pinn(test_model, data_tuple, epochs=2, use_lbfgs=False)\n",
    "\n",
    "# âœ… Check your answer\n",
    "# Analyze the training function to extract important components\n",
    "import inspect\n",
    "\n",
    "train_fn_code = inspect.getsource(train_pinn)\n",
    "\n",
    "# Check for key components in the code\n",
    "has_adam = 'Adam' in train_fn_code\n",
    "has_lbfgs = 'LBFGS' in train_fn_code\n",
    "has_closure = 'def closure' in train_fn_code\n",
    "has_data_loss = 'data_loss' in train_fn_code\n",
    "has_physics_loss = 'physics_loss' in train_fn_code\n",
    "computes_residuals = 'compute_ns_residuals' in train_fn_code\n",
    "updates_weights = 'optimizer.step()' in train_fn_code\n",
    "backprop = 'backward()' in train_fn_code\n",
    "\n",
    "answer = {\n",
    "    'function_code': train_fn_code,  # For deeper inspection\n",
    "    'has_adam': has_adam,\n",
    "    'has_lbfgs': has_lbfgs,\n",
    "    'uses_closure': has_closure,\n",
    "    'has_data_loss': has_data_loss,\n",
    "    'has_physics_loss': has_physics_loss,\n",
    "    'computes_residuals': computes_residuals,\n",
    "    'updates_weights': updates_weights,\n",
    "    'uses_backpropagation': backprop,\n",
    "    'learning_rate': 0.001 if 'lr=0.001' in train_fn_code else None,\n",
    "    'optimizer_params': {\n",
    "        'has_max_iter': 'max_iter=' in train_fn_code,\n",
    "        'has_line_search': 'line_search_fn=' in train_fn_code\n",
    "    }\n",
    "}\n",
    "checker.check_exercise(9, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with Adam:   2%|â–         | 197/10000 [00:12<10:21, 15.77it/s, loss=0.1107]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model = NavierStokesPINN(hidden_size=\u001b[32m20\u001b[39m, num_layers=\u001b[32m9\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m trained_model = \u001b[43mtrain_pinn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_lbfgs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[32m     13\u001b[39m torch.save(trained_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33mcylinder_pinn_model.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain_pinn\u001b[39m\u001b[34m(model, data, epochs, use_lbfgs)\u001b[39m\n\u001b[32m     44\u001b[39m pbar = tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc=\u001b[33m\"\u001b[39m\u001b[33mTraining with Adam\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     optimizer.step()\n\u001b[32m     48\u001b[39m     pbar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain_pinn.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Total loss\u001b[39;00m\n\u001b[32m     38\u001b[39m loss = data_loss + physics_loss\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Corona\\Documents\\Git\\UoM_fse_dl_workshop\\torch_fse\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Corona\\Documents\\Git\\UoM_fse_dl_workshop\\torch_fse\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Corona\\Documents\\Git\\UoM_fse_dl_workshop\\torch_fse\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = NavierStokesPINN(hidden_size=20, num_layers=9)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_pinn(\n",
    "    model,\n",
    "    (x_train, y_train, t_train, u_train, v_train),\n",
    "    epochs=10000,\n",
    "    use_lbfgs=True\n",
    ")\n",
    "\n",
    "# save the model\n",
    "torch.save(trained_model.state_dict(), 'cylinder_pinn_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Evaluation\n",
    "\n",
    "Unlike standard models where performance is often quantified with scalar metrics like Mean Squared Error (MSE) or R-squared, PINNs are typically evaluated by comparing predicted and true physical fields over time (e.g., velocity and pressure fields in fluid dynamics).\n",
    "\n",
    "Here, we generate predictions from the trained model across all time steps and compare them to the ground truth values. Since we're modeling time-dependent flow behavior, we loop over each time snapshot and feed spatial coordinates (x, y) along with the corresponding time value t into the model. We compute and collect the predicted fields:\n",
    "\n",
    "- `u_pred`: Predicted horizontal velocity\n",
    "\n",
    "- `v_pred`: Predicted vertical velocity\n",
    "\n",
    "- `p_pred`: Predicted pressure\n",
    "\n",
    "These are then compared against the ground truth data from the test set for visual inspection and potential error metrics like relative L2 norm or RMSE (if applicable).\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: When passing the data to the model, we need to ensure the tensors have `requires_grad=True`. This is important for the autograd engine to track operations on these tensors and compute gradients correctly. This is especially crucial when using optimisers like L-BFGS, which rely on gradient information to update model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "âœ… u_pred_shape is correct!\n",
      "âœ… v_pred_shape is correct!\n",
      "âœ… p_pred_shape is correct!\n",
      "âœ… requires_grad_used is correct!\n",
      "\n",
      "ðŸŽ‰ Excellent! All parts are correct!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eef6e826994829928448b355086e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Time Step', max=199), Output()), _dom_classes=('widget-iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b7889b88c74da792776ae7ae63c021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 10: Inference and Visualization ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Load the trained model\n",
    "# 2. Set the model to evaluation mode\n",
    "# 3. Move the model to CPU for inference\n",
    "# 4. Create input tensors with requires_grad=True\n",
    "# 5. Get predictions for u, v, and p\n",
    "# 6. Store predictions in lists and convert to arrays\n",
    "# 7. Reshape ground truth data\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu') # Move model to CPU for inference\n",
    "with torch.no_grad():\n",
    "    u_pred = []\n",
    "    v_pred = []\n",
    "    p_pred = []\n",
    "    \n",
    "    for t_idx in range(time.shape[0]):\n",
    "        # Create input tensors with requires_grad=True\n",
    "        x_tensor = torch.FloatTensor(x_star[:,0]).view(-1, 1).requires_grad_(True)\n",
    "        y_tensor = torch.FloatTensor(x_star[:,1]).view(-1, 1).requires_grad_(True)\n",
    "        t_tensor = torch.full_like(x_tensor, time[t_idx,0]).requires_grad_(True)\n",
    "        \n",
    "        # Get predictions - temporarily enable gradients\n",
    "        with torch.enable_grad():\n",
    "            u_t, v_t, p_t = model(x_tensor, y_tensor, t_tensor)\n",
    "        \n",
    "        # Store predictions\n",
    "        u_pred.append(u_t.detach().cpu().numpy())\n",
    "        v_pred.append(v_t.detach().cpu().numpy())\n",
    "        p_pred.append(p_t.detach().cpu().numpy())\n",
    "    \n",
    "    # Convert to arrays and reshape\n",
    "    u_pred = np.stack(u_pred, axis=1)  # Shape: (N, T)\n",
    "    v_pred = np.stack(v_pred, axis=1)  # Shape: (N, T)\n",
    "    p_pred = np.stack(p_pred, axis=1)  # Shape: (N, T)\n",
    "\n",
    "# Reshape ground truth data\n",
    "u_true = u_star[:, 0, :]  # Shape: (N, T)\n",
    "v_true = u_star[:, 1, :]  # Shape: (N, T)\n",
    "p_true = p_star  # Shape: (N, T)\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'u_pred_shape': u_pred.shape,\n",
    "    'v_pred_shape': v_pred.shape,\n",
    "    'p_pred_shape': p_pred.shape,\n",
    "    'requires_grad_used': True\n",
    "}\n",
    "checker.check_exercise(10, answer)\n",
    "\n",
    "# Create visualization\n",
    "utils.plotting.visualize_flow_comparison_interactive(\n",
    "    x_star, u_true, v_true, p_true,\n",
    "    u_pred, v_pred, p_pred, time,\n",
    "    figsize=(18, 4), \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_fse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
