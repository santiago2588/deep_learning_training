{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/se_03.png)\n",
    "# Workshop Instructions\n",
    "***\n",
    "- <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> Follow along by typing the code yourself - this helps with learning!\n",
    "- <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> Code cells marked as \"Exercise\" are for you to complete\n",
    "- <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(1500%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> Look for hints if you get stuck\n",
    "- <img src=\"figs/icons/success.svg\" width=\"20\" style=\"filter: invert(56%) sepia(71%) saturate(5293%) hue-rotate(117deg) brightness(95%) contrast(101%);\"/> Compare your solution with the provided answers\n",
    "- <img src=\"figs/icons/list.svg\" width=\"20\" style=\"filter: invert(19%) sepia(75%) saturate(6158%) hue-rotate(312deg) brightness(87%) contrast(116%);\"/> Don't worry if you make mistakes - debugging is part of learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "helper_utils = Path.cwd().parent\n",
    "sys.path.append(str(helper_utils))\n",
    "\n",
    "\n",
    "checker = utils.core.ExerciseChecker(\"SE03\")\n",
    "quizzer = utils.core.QuizManager(\"SE03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PyTorch workflow\n",
    "***\n",
    "The previous session we had a look at the basics of neural networks and how to train a single layer perceptron. In this session we will look at the PyTorch framework and how to use it to build and train neural networks.\n",
    "\n",
    "Most deep learning projects follow a similar workflow. The following figure illustrates the typical workflow of a PyTorch project:\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs\\pytorch_workflow.png\" alt=\"PyTorch WorkFlow\" align=\"center\" style=\"width: 80%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "The workflow consists of the following steps:\n",
    "\n",
    "| Step | Description |\n",
    "|------|----------|\n",
    "| Obtain Data | Collect and preprocess the data for training and testing |\n",
    "| Prepare Data | Setup data in PyTorch format |\n",
    "| Pre-process Data | Normalize and augment the data. This may involve data cleaning, normalization, and splitting the data into training, validation, and test sets. |\n",
    "| Activation Function | Choose an activation function for the model. This may involve selecting a suitable activation function for the model, such as ReLU, sigmoid, or tanh. |\n",
    "| Model | Define the model architecture. |\n",
    "| Choose optimiser | Select an optimiser for the model. |\n",
    "| Choose loss function | Select a loss function for the model. |\n",
    "| Create training loop | Define the training steps, including forward pass, backward pass, and parameter updates. |\n",
    "| Fit model | Train the model using the training data. |\n",
    "| Evaluate model | Evaluate the model using the validation and test data to make predictions |\n",
    "| Improve model | Fine-tune the model by adjusting hyperparameters, adding regularization, or modifying the architecture. |\n",
    "| Save or deploy model | Save the trained model for future use or deploy it in a production environment. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Obtain Data\n",
    "***\n",
    "In this notebook we are going to explore a realistic scenario of incompressible fluid flow as described by the ubiquitous Navier-Stokes equations. Navier-Stokes equations describe the physics of many phenomena of scientific and engineering. Often, the Navier-Stokes equations are solved using numerical methods, such as finite element or finite volume methods. However, these methods can be computationally expensive and time-consuming, especially for complex geometries and boundary conditions.\n",
    "In this workshop, we will use a dataset of incompressible fluid flow around a cylinder. The dataset is generated using a finite volume method and contains the velocity and pressure fields of the fluid flow. The dataset consists of the following variables:\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| u        | x-component of velocity |\n",
    "| v        | y-component of velocity |\n",
    "| p        | pressure |\n",
    "| t        | time |\n",
    "\n",
    "The dataset is generated usign the spectral/hp-element solver NekTar. The solution domain is discretised in space by a tessellation consisting of 412 triangular elements. It is assumed a uniform free stream velocity profile imposed at the left boundary, a zero pressure outflow condition imposed at the right boundary located 25 diameters downstream of the cylinder, and periodicity for the top and bottom boundaries of the [‚àí15, 25] √ó [‚àí8, 8] domain.\n",
    "\n",
    "For this problem, we want to predict the Convective term $\\lambda_1$, the viscous term $\\lambda_2$, as well as a reconstruction of the pressure field $p$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:\n",
      "Cylinder dataset for predicting the drag coefficient of a cylinder in a flow field\n",
      "> Authors: Maziar Raissi1, Paris Perdikaris, George Em Karniadakis\n",
      "> Year: 2017\n",
      "> Website: https://arxiv.org/pdf/1711.10566.pdf\n",
      "\n",
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('cylinder',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=False,\n",
    "                                   remove_compressed=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5000 points in space, 2 dimensions and 200 time steps.\n"
     ]
    }
   ],
   "source": [
    "data = scipy.io.loadmat(dataset_path)\n",
    "u_star = data['U_star'] # velocity n x 2 x time\n",
    "p_star = data['p_star'] # pressure n x time\n",
    "x_star = data['X_star'] # coordinates n x 2\n",
    "time = data['t'] # time n x 1\n",
    "\n",
    "print(f'We have {u_star.shape[0]} points in space, {u_star.shape[1]} dimensions and {u_star.shape[2]} time steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6e7f64ca14aa48dfc14e1cee32bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Time Step', max=199), Output()), _dom_classes=('widget-i‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plotting.wake_cylinder(x_star, u_star, p_star, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 and 3: Prepare and Pre-process Data\n",
    "***\n",
    "The next step is to pre-process the data. This involves normalizing the data and splitting it into training, validation, and test sets.\n",
    "\n",
    "### Training, Validation, and Test Sets\n",
    "***\n",
    "One of the crucial steps in machine learning is to split the data into training, validation, and test sets. Each of these sets serves a specific purpose in the model development process:\n",
    "\n",
    "| Dataset | Purpose | Typical Split | Usage | Analogy |\n",
    "|---------|---------|---------------|--------|----------|\n",
    "| Training Set | Used to train the model by adjusting weights and biases through backpropagation | 60-80% | Every training iteration | Like studying materials to learn a subject |\n",
    "| Validation Set | Used to tune hyperparameters and monitor model performance during training to prevent overfitting | 10-20% | During model development | Like practice exams to gauge learning progress |\n",
    "| Test Set | Used only once for final model evaluation; never used for training or tuning | 10-20% | Once, after training | Like a final exam with new, unseen questions |\n",
    "***\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Note**: We are going to use only 1% of the total dataset for training (the authors have already prepared the data in this way). This is to highlight the ability of PINNs to learn from limited data. In practice, you would typically use a larger portion of the dataset for training. Thus, we are not going to split the dataset into training, validation, and test sets. Instead, we will use the entire dataset for training and testing. This is a common practice in PINNs, where the model is trained on a small portion of the data and then tested on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T = x_star.shape[0], time.shape[0] # number of points in space and time\n",
    "\n",
    "# Create coordinate grids \n",
    "x_flat = x_star[:, 0]  # Extract x coordinates\n",
    "y_flat = x_star[:, 1]  # Extract y coordinates\n",
    "t_flat = time.flatten() # Flatten time array\n",
    "\n",
    "# Create meshgrid-like structures for visualization\n",
    "x_coords = np.tile(x_flat[:, np.newaxis], (1, T))  # Repeat x for each timestep\n",
    "y_coords = np.tile(y_flat[:, np.newaxis], (1, T))  # Repeat y for each timestep\n",
    "time_coords = np.tile(t_flat, (N, 1))              # Repeat time for each spatial point\n",
    "\n",
    "# Extract velocity and pressure data\n",
    "u_vals = u_star[:, 0, :]  # Extract u velocity\n",
    "v_vals = u_star[:, 1, :]  # Extract v velocity\n",
    "p_vals = p_star[:, :]     # Extract pressure\n",
    "\n",
    "# Flatten into NT x 1 arrays\n",
    "x = x_coords.flatten()[:, np.newaxis]\n",
    "y = y_coords.flatten()[:, np.newaxis]\n",
    "t = time_coords.flatten()[:, np.newaxis]\n",
    "u = u_vals.flatten()[:, np.newaxis]\n",
    "v = v_vals.flatten()[:, np.newaxis]\n",
    "p = p_vals.flatten()[:, np.newaxis]\n",
    "\n",
    "idx = np.random.choice(N*T, N, replace=False)\n",
    "x_train = x[idx, :]\n",
    "y_train = y[idx, :]\n",
    "t_train = t[idx, :]\n",
    "u_train = u[idx, :]\n",
    "v_train = v[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "***\n",
    "Normalisation is a crucial step in the pre-processing of data for machine learning models. It involves scaling the input features to a similar range, which helps improve the convergence speed and performance of the model. In this notebook, we will use scale the data to the range [-1, 1] using Min-Max scaling. This is a common technique used in machine learning to ensure that all features contribute equally to the model's performance.\n",
    "Min-Max scaling transforms the data to a fixed range, typically [0, 1] or [-1, 1]. The formula for Min-Max scaling is:\n",
    "$$\n",
    "X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}} \\cdot (max - min) + min\n",
    "$$\n",
    "where:\n",
    "- $X$ is the original data\n",
    "- $X_{min}$ and $X_{max}$ are the minimum and maximum values of the feature, respectively\n",
    "- $min$ and $max$ are the desired minimum and maximum values of the scaled data, respectively\n",
    "\n",
    "The normalisation parameters will be computed from the training set and then applied to the validation and test sets. This helps to prevent data leakage and ensures that the model is evaluated on unseen data. \n",
    "\n",
    "| Benefit | Description | Impact on Training |\n",
    "|---------|-------------|-------------------|\n",
    "| **Faster Convergence** | Normalized inputs lead to better-conditioned optimization | Reduces training time |\n",
    "| **Numerical Stability** | Prevents extremely large or small values | Reduces risk of gradient explosions/vanishing |\n",
    "| **Feature Scaling** | Makes all features contribute equally to the model | Prevents certain features from dominating |\n",
    "| **Better Generalization** | Helps models transfer between different images | Improves performance on unseen data |\n",
    "\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 1**: Normalisation using Min-Max scaling\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the training\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Inverse transform the scaled data to get the original values\n",
    "X_train_original = scaler.inverse_transform(X_train_scaled)\n",
    "```\n",
    "\n",
    "> <img src=\"figs/icons/list.svg\" width=\"20\" style=\"filter: invert(19%) sepia(75%) saturate(6158%) hue-rotate(312deg) brightness(87%) contrast(116%);\"> **Important**: For PINNs, the features cannot be normalised in the same way as in traditional machine learning. Standard normalization in PINNs is tricky because physical equations must remain consistent with the scaled variables. The normalization should be applied to the loss function considering the physics of the problem. This is a more complex process and requires a deeper understanding of the physical equations involved. In this workshop, we will not cover this topic in detail, but it is important to keep in mind that normalisation in PINNs is not as straightforward as in traditional machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data as tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "t_train = torch.tensor(t_train, dtype=torch.float32)\n",
    "u_train = torch.tensor(u_train, dtype=torch.float32)\n",
    "v_train = torch.tensor(v_train, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Activation Function\n",
    "***\n",
    "The next step is to choose an activation function for the model. The activation function introduces non-linearity to the model, allowing it to learn complex relationships in the data. The following table lists some common activation functions used in neural networks, along with their characteristics and best use cases:\n",
    "\n",
    "| Function | Formula | Range | PyTorch Implementation | Best Used For |\n",
    "|----------|---------|-------|-------------------|---------------|\n",
    "| ReLU | $f(x) = \\max(0, x)$ | $[0, \\infty)$ | `torch.nn.ReLU()` | Hidden layers in most networks |\n",
    "| Sigmoid | $f(x) = \\frac{1}{1+e^{-x}}$ | $(0, 1)$ | `torch.nn.Sigmoid()` | Binary classification, gates in LSTMs |\n",
    "| Tanh | $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $(-1, 1)$ | `torch.nn.Tanh()` | Hidden layers when output normalization is needed |\n",
    "| Leaky ReLU | $f(x) = \\max(\\alpha x, x)$ | $(-\\infty, \\infty)$ | `torch.nn.LeakyReLU(negative_slope=0.01)` | Preventing \"dead neurons\" problem |\n",
    "| Softmax | $f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$ | $(0, 1)$ | `torch.nn.Softmax(dim=1)` | Multi-class classification output layer |\n",
    "\n",
    "The choice of activation function depends on the specific problem and the architecture of the neural network. \n",
    "\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(1500%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> - ReLU is the most commonly used activation function in hidden layers of deep networks due to its simplicity and effectiveness.\n",
    "> - The activation function for the output layer depends on the type of problem being solved (e.g., regression, binary classification, multi-class classification).\n",
    "***\n",
    "> <img src=\"figs/icons/list.svg\" width=\"20\" style=\"filter: invert(19%) sepia(75%) saturate(6158%) hue-rotate(312deg) brightness(87%) contrast(116%);\"/> **Common Mistakes to Avoid**: \n",
    "> - Mixing activation functions in the same layer (e.g., using ReLU and sigmoid together) can lead to unexpected behavior.\n",
    "> - Using activation functions that saturate (like sigmoid) in hidden layers can lead to vanishing gradients, making training difficult.\n",
    "> - Forgetting to apply the activation function to the output layer can lead to incorrect predictions (e.g., not using softmax for multi-class classification).\n",
    "> - Not considering the range of the output when choosing the activation function (e.g., using sigmoid for regression tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß† Quiz 1: Choosing the right activation function\")\n",
    "quizzer.run_quiz(1)\n",
    "\n",
    "print(\"\\nüß† Quiz 2: Combining activation functions\")\n",
    "quizzer.run_quiz(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Model\n",
    "***\n",
    "The next step is to define the model architecture. In order to create a Neural Network we need to stack multiple neurons together. This is known as a **layer**. A layer is a collection of neurons that work together to process the input data. A simple ANN is formed by three types of layers:\n",
    "   - **Input Layer**: Receives the input data.\n",
    "   - **Hidden Layers**: Intermediate layers that process the data.\n",
    "   - **Output Layer**: Produces the final output.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs\\layers.png\" alt=\"ANN layers\" align=\"center\" style=\"width: 20%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "The following table summarises the different types of layers available in PyTorch:\n",
    "\n",
    "| Layer Type | Class | Description | Common Uses |\n",
    "|------------|-------|-------------|------------|\n",
    "| Fully Connected | `torch.nn.Linear(in_features, out_features)` | Standard dense layer | Classification, regression |\n",
    "| Convolutional | `torch.nn.Conv2d(in_channels, out_channels, kernel_size)` | Spatial feature extraction | Image processing |\n",
    "| Recurrent | `torch.nn.RNN(input_size, hidden_size)` | Sequential data processing | Time series, text |\n",
    "| LSTM | `torch.nn.LSTM(input_size, hidden_size)` | Long-term dependencies | Complex sequences |\n",
    "| Embedding | `torch.nn.Embedding(num_embeddings, embedding_dim)` | Word vector representations | NLP tasks |\n",
    "| BatchNorm | `torch.nn.BatchNorm2d(num_features)` | Normalizes layer inputs | Training stability |\n",
    "| Dropout | `torch.nn.Dropout(p=0.5)` | Randomly zeros elements | Regularization |\n",
    "\n",
    "The choice of layer type depends on the specific problem and the architecture of the neural network. For example, convolutional layers are commonly used in image processing tasks, while recurrent layers are used for sequential data processing.\n",
    "\n",
    "### Number of Layers and Neurons\n",
    "***\n",
    "The number of layers and neurons in each layer is a hyperparameter that needs to be tuned. The following table summarises the common practices for choosing the number of layers and neurons:\n",
    "\n",
    "| Layer Type | Common Practices |\n",
    "|----------------|------------------|\n",
    "| Input Layer | Number of neurons = number of input features |\n",
    "| Hidden Layers | 1-3 hidden layers are common for most tasks. More complex tasks may require more layers. |\n",
    "| Output Layer | Number of neurons = number of output features (e.g., 1 for regression, number of classes for classification) |\n",
    "| Number of Neurons | Common practices: 2^n, where n is the number of layers. A common practice is to start with a number of neurons equal to the number of input features and then reduce the number of neurons in each subsequent layer. |\n",
    "***\n",
    "\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> - Start with a simple architecture and gradually increase complexity as needed.\n",
    "> - The number of neurons in each layer can be adjusted based on the complexity of the problem.\n",
    "> - Use activation functions after each layer to introduce non-linearity.\n",
    "> - Experiment with different layer types and configurations to find the best architecture for your problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz 3: Network Width\n",
    "print(\"\\nüß† Quiz 3: Understanding Network Width for Inverse Kinematics\")\n",
    "quizzer.run_quiz(3)\n",
    "\n",
    "# Quiz 4: Network Depth\n",
    "print(\"\\nüß† Quiz 4: Understanding Network Depth for Inverse Kinematics\")\n",
    "quizzer.run_quiz(4)\n",
    "\n",
    "# Quiz 5: Regularization Techniques\n",
    "print(\"\\nüß† Quiz 5: Regularization Techniques for Kinematics Models\")\n",
    "quizzer.run_quiz(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising Weights and Biases\n",
    "***\n",
    "\n",
    "In the previous session we looked at the concept of weights and biases. With our Perceptron we initialised the weights and biases to random values. In PyTorch, we can use different methods to initialise the weights and biases of a neural network.\n",
    "\n",
    "The importance of initialising weights and biases lies in the fact that they can significantly affect the convergence speed and performance of the neural network. Proper initialisation can help prevent issues such as vanishing or exploding gradients, which can hinder the training process.\n",
    "\n",
    "| Initialisation Method | Formula | PyTorch Code | Description |\n",
    "|-----------------------|----------|--------------|-------------|\n",
    "| Xavier/Glorot Initialisation | $W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$ | `torch.nn.init.xavier_uniform_(tensor)` | Suitable for sigmoid and tanh activations. |\n",
    "| He Initialisation | $W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})$ | `torch.nn.init.kaiming_uniform_(tensor)` | Suitable for ReLU activations. |\n",
    "| Kaiming Normal Initialisation | $W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in}}})$ | `torch.nn.init.kaiming_normal_(tensor)` | Suitable for ReLU activations. |\n",
    "| Kaiming Uniform Initialisation | $W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})$ | `torch.nn.init.kaiming_uniform_(tensor)` | Suitable for ReLU activations. |\n",
    "| Zero Initialisation | $W = 0$ | `torch.nn.init.zeros_(tensor)` | All weights are set to zero. Not recommended. |\n",
    "| Random Initialisation | $W \\sim \\mathcal{U}(-1, 1)$ | `torch.nn.init.uniform_(tensor)` | Weights are randomly initialised between -1 and 1. |\n",
    "***\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> - Use Xavier or He initialisation for most cases, as they are designed to maintain the variance of activations across layers.\n",
    "> - Avoid zero initialisation, as it can lead to symmetry problems where all neurons learn the same features.\n",
    "> - PyTorch uses Kaiming initialisation by default for `torch.nn.Linear` layers, which is suitable for ReLU activations.\n",
    "> - Experiment with different initialisation methods to see their impact on training speed and model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PINN Model\n",
    "***\n",
    "In this workshop we are going to use a Physics-Informed Neural Network (PINN) to solve the Navier-Stokes equations. The PINN model is a neural network that is trained to satisfy the Navier-Stokes equations, as well as the boundary conditions of the problem. The PINN model consists of the following components:\n",
    "\n",
    "| Component | Description |\n",
    "|----------|-------------|\n",
    "| Neural Network | A feedforward neural network that takes the input features (e.g., coordinates, time) and outputs the predicted values (e.g., velocity, pressure). |\n",
    "| Loss Function | A combination of the mean squared error (MSE) loss and the physics loss, which is based on the Navier-Stokes equations. |\n",
    "| Optimiser | An optimisation algorithm (e.g., Adam, SGD) that updates the weights and biases of the neural network during training. |\n",
    "\n",
    "For the connected layers we are going to use the following activation functions:\n",
    "| Layer | Activation Function |\n",
    "|-------|---------------------|\n",
    "| Input Layer | Tanh |\n",
    "| Hidden Layers | Tanh |\n",
    "| Output Layer | Tanh |\n",
    "\n",
    "The choice of activation function for the output layer is important, as it can affect the range of the output values. In this case, we are using Tanh to ensure that the output values are in the range [-1, 1]. This is important for the PINN model, as we want to ensure that the predicted values are in the same range as the input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Model Creation with Weight Initialization üéØ\n",
    "class CylinderPINN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=20, num_layers=9):\n",
    "        super().__init__()\n",
    "        self.nu = 0.01  # kinematic viscosity\n",
    "        \n",
    "        # Neural network architecture\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Linear(3, hidden_size))  # Input layer: x, y, t\n",
    "        layers.append(torch.nn.Tanh())\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(torch.nn.Tanh())\n",
    "            \n",
    "        layers.append(torch.nn.Linear(hidden_size, 2))  # Output layer: œà (stream function), p (pressure)\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        # Combine inputs\n",
    "        inputs = torch.cat([x, y, t], dim=1)\n",
    "        # Get stream function (œà) and pressure (p)\n",
    "        outputs = self.net(inputs)\n",
    "        psi, p = outputs[:, 0:1], outputs[:, 1:2]\n",
    "        \n",
    "        # Calculate velocities and their derivatives\n",
    "        u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "        v = -torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "        \n",
    "        return u, v, p\n",
    "\n",
    "model = CylinderPINN(hidden_size=20, num_layers=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Choose Optimiser\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> **Definition**: Optimisers are algorithms used to update the model parameters during training to minimise the loss function.\n",
    "\n",
    "The next step is to choose an optimiser for the model.\n",
    "\n",
    "\n",
    "\n",
    "The optimiser algorithm is used to update the model parameters during training. Most optimizers use a version of gradient descent to update the model parameters. The goal of the optimiser is to minimize the loss function by adjusting the weights and biases of the model. The most commonly used optimizers include:\n",
    "\n",
    "| Optimizer | PyTorch Implementation | Best Used For |\n",
    "|-----------|---------------------|--------------|\n",
    "| Stochastic Gradient Descent (SGD) | `torch.optim.SGD(params, lr)` | Simple problems, good with momentum |\n",
    "| Adam | `torch.optim.Adam(params, lr)` | Most deep learning tasks |\n",
    "| RMSProp | `torch.optim.RMSprop(params, lr)` | Deep neural networks |\n",
    "| Adagrad | `torch.optim.Adagrad(params, lr)` | Sparse data tasks |\n",
    "| AdamW | `torch.optim.AdamW(params, lr)` | When regularization is important |\n",
    "| LBFGS | `torch.optim.LBFGS(params)` | Small datasets, expensive computations |\n",
    "\n",
    "The Adam optimiser is a popular choice for training deep learning models due to its efficiency and effectiveness. It combines the benefits of both SGD and RMSProp, making it suitable for a wide range of tasks.\n",
    "\n",
    "### Learning Rate\n",
    "***\n",
    "The learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function. A small learning rate may lead to slow convergence, while a large learning rate may cause the model to diverge. It is important to choose an appropriate learning rate for the optimizer to work effectively\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs\\learning_rate.png\" alt=\"Gradient Descent\" align=\"center\" style=\"width: 30%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "## PINN Optimiser\n",
    "***\n",
    "When training a PINN model, the choice of optimiser is crucial for achieving good performance. While the Adam optimiser is a popular choice for many deep learning tasks, it may not always be the best choice for PINNs. A better choice for PINNs is the L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) optimiser, which is a quasi-Newton method that approximates the inverse Hessian matrix. L-BFGS is particularly well-suited for PINNs because it can efficiently handle the large number of parameters in the model and can converge faster than Adam in some cases.\n",
    "\n",
    "However, in order to use L-BFGS, we need to ensure that the loss function is differentiable with respect to the model parameters. This is important because L-BFGS uses second-order information (the Hessian) to update the model parameters. If the loss function is not differentiable, L-BFGS may not converge or may converge to a suboptimal solution. \n",
    "\n",
    "To use the L-BFGS optimiser in PyTorch, we need to define a closure function that computes the loss and gradients. The closure function is called by the L-BFGS optimiser to compute the loss and gradients at each iteration. The closure function should return the loss value and the gradients of the model parameters with respect to the loss.\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 2**: Using L-BFGS Optimiser\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "model = PINNModel()\n",
    "# Define the loss function\n",
    "loss_function = PINNLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.01)\n",
    "# Define the closure function\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(model)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.step(closure)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step 7: Choose Loss Function\n",
    "***\n",
    "The next step is to choose a loss function for the model. The choice of loss function depends on the type of problem being solved. The loss function measures how well the model is performing and guides the optimisation process. The most commonly used loss functions include:\n",
    "\n",
    "| Loss Function | PyTorch Implementation | Best Used For |\n",
    "|---------------|---------------------|--------------|\n",
    "| Mean Squared Error (MSE) | `torch.nn.MSELoss()` | Regression tasks |\n",
    "| Mean Absolute Error (MAE) | `torch.nn.L1Loss()` | Regression tasks |\n",
    "| Binary Cross-Entropy | `torch.nn.BCELoss()` | Binary classification tasks |\n",
    "| Categorical Cross-Entropy | `torch.nn.CrossEntropyLoss()` | Multi-class classification tasks |\n",
    "| Hinge Loss | `torch.nn.HingeEmbeddingLoss()` | Support Vector Machines (SVM) |\n",
    "| Kullback-Leibler Divergence | `torch.nn.KLDivLoss()` | Probabilistic models |\n",
    "\n",
    "The loss works in conjunction with the optimiser. While there are loss functions that can work for the same task, the choice of loss will have an effect on the final performance of the model. For instance, using MSE (L2-Norm) loss for a regression task will penalise larger errors more than smaller ones, while MAE (L1-Norm) loss treats all errors equally. This can lead to different model performance depending on the distribution of the data.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs\\losses.png\" alt=\"Loss Functions\" align=\"center\" style=\"width: 30%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Tips**:\n",
    "> - Choose a loss function that is appropriate for the type of problem being solved (e.g., regression, classification).\n",
    "> - Monitor the loss during training to ensure that the model is converging and not overfitting.\n",
    "> - Experiment with different loss functions to see their impact on model performance.\n",
    "\n",
    "## PINN Loss Function\n",
    "***\n",
    "The loss function for the PINN model is a combination of the mean squared error (MSE) loss and the physics loss, which is based on the Navier-Stokes equations. The total loss function can be expressed as:\n",
    "$$\n",
    "L_{total} = L_{MSE} + \\lambda_1 L_{convective} + \\lambda_2 L_{viscous} + \\lambda_3 L_{pressure}\n",
    "$$\n",
    "where:\n",
    "- $L_{MSE}$ is the mean squared error loss between the predicted and true values\n",
    "- $L_{convective}$ is the loss associated with the convective term of the Navier-Stokes equations\n",
    "- $L_{viscous}$ is the loss associated with the viscous term of the Navier-Stokes equations\n",
    "- $L_{pressure}$ is the loss associated with the pressure term of the Navier-Stokes equations\n",
    "- $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ are the weights for each term in the loss function. These weights can be adjusted to balance the contributions of each term to the total loss. The choice of these weights is important, as they can significantly affect the performance of the PINN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Optimizer and Loss Function Selection üéØ\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def compute_pde_loss(model, x, y, t):\n",
    "    u, v, p = model(x, y, t)\n",
    "    \n",
    "    # Calculate all required derivatives\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    \n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "    \n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    \n",
    "    # Navier-Stokes equations residuals\n",
    "    f = u_t + u * u_x + v * u_y + p_x - model.nu * (u_xx + u_yy)\n",
    "    g = v_t + u * v_x + v * v_y + p_y - model.nu * (v_xx + v_yy)\n",
    "    \n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 and 9: Create Training Loop and Fit Model\n",
    "***\n",
    "The training loop implements the key steps for training a neural network model:\n",
    "\n",
    "| Step | Description | Code Example |\n",
    "|------|-------------|--------------|\n",
    "| 1. Forward Pass | Pass input data through model to generate predictions | `predictions = model(inputs)` |  \n",
    "| 2. Loss Computation | Calculate loss between predictions and targets | `loss = criterion(predictions, targets)` |\n",
    "| 3. Backward Pass | Compute gradients through backpropagation | `loss.backward()` |\n",
    "| 4. Parameter Updates | Update model parameters using optimizer | `optimizer.step()` |\n",
    "| 5. Gradient Reset | Zero out gradients for next iteration | `optimizer.zero_grad()` |\n",
    "\n",
    "The next step is to fit the model using the training data. The model is trained for a specified number of epochs, and the training and validation loss is monitored during training. The number of epochs is a hyperparameter that determines how many times the model will be trained on the entire training dataset.\n",
    "\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 3**: Training Loop Structure\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Forward Pass\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # 2. Loss Computation\n",
    "    loss = criterion(predictions, targets)\n",
    "    \n",
    "    # 3. Backward Pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Parameter Updates\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 5. Gradient Reset\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PINN:  14%|‚ñà‚ñç        | 1406/10000 [04:53<1:18:31,  1.82it/s, data_loss=0.0636, physics_loss=0.0048, total_loss=0.0684]"
     ]
    }
   ],
   "source": [
    "def train_pinn(model, data, epochs=20000, lr=0.001):\n",
    "    x, y, t, u_true, v_true = data\n",
    "    \n",
    "    # Check if inputs are already tensors, if not convert them\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "    if not isinstance(t, torch.Tensor):\n",
    "        t = torch.tensor(t, dtype=torch.float32)\n",
    "    if not isinstance(u_true, torch.Tensor):\n",
    "        u_true = torch.tensor(u_true, dtype=torch.float32)\n",
    "    if not isinstance(v_true, torch.Tensor):\n",
    "        v_true = torch.tensor(v_true, dtype=torch.float32)\n",
    "        \n",
    "    # Set requires_grad=True for inputs to compute gradients\n",
    "    x.requires_grad_(True)\n",
    "    y.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    # Use tqdm for the progress bar\n",
    "    pbar = tqdm(range(epochs), desc=\"Training PINN\")\n",
    "    \n",
    "    # Lists to store losses for plotting\n",
    "    data_losses = []\n",
    "    physics_losses = []\n",
    "    total_losses = []\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Data loss\n",
    "        u_pred, v_pred, _ = model(x, y, t)\n",
    "        data_loss = mse_loss(u_pred, u_true) + mse_loss(v_pred, v_true)\n",
    "        \n",
    "        # Physics loss\n",
    "        f, g = compute_pde_loss(model, x, y, t)\n",
    "        physics_loss = mse_loss(f, torch.zeros_like(f)) + mse_loss(g, torch.zeros_like(g))\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = data_loss + physics_loss\n",
    "        \n",
    "        # Store losses\n",
    "        data_losses.append(data_loss.item())\n",
    "        physics_losses.append(physics_loss.item())\n",
    "        total_losses.append(total_loss.item())\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the progress bar with current losses\n",
    "        pbar.set_postfix({\n",
    "            'data_loss': f'{data_loss.item():.4f}',\n",
    "            'physics_loss': f'{physics_loss.item():.4f}',\n",
    "            'total_loss': f'{total_loss.item():.4f}'\n",
    "        })\n",
    "        \n",
    "    # Plot the losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(data_losses, label='Data Loss')\n",
    "    plt.semilogy(physics_losses, label='Physics Loss')\n",
    "    plt.semilogy(total_losses, label='Total Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('PINN Training Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = CylinderPINN(hidden_size=20, num_layers=9)\n",
    "trained_model = train_pinn(model, (x_train, y_train, t_train, u_train, v_train), epochs=10000, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, Underfitting, and Early Stopping\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> **Definition**: Overfitting occurs when a model learns the training data too well, including noise and outliers, leading to poor generalisation on unseen data. Underfitting occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "As we can see in the following figure, the training loss decreases over time, while the validation loss follows a similar trend. However, the validation loss starts to slowly deviate from the training loss after a certain number of epochs. This indicates that the model is starting to overfit the training data. The point at which the validation loss starts to increase is known as the \"early stopping\" point. This is the point at which we should stop training the model to prevent overfitting.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs\\over_under_fit.png\" alt=\"Overfitting\" align=\"center\" style=\"width: 60%; height: auto; margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Model\n",
    "***\n",
    "The next step is to evaluate the model using the validation and test data. The model is evaluated on the validation set during training to monitor its performance and prevent overfitting. \n",
    "\n",
    "Since we are training a model with MSE loss, we can also plot the predicted output against the actual output to see how well the model is performing. The predicted output should be close to the actual output, and the points should be clustered around the diagonal line. If the points are scattered far from the diagonal line, it indicates that the model is not performing well.\n",
    "\n",
    "We can also compute the R-squared value to quantify the performance of the model. The R-squared value is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. The R-squared value ranges from 0 to 1, where 0 indicates that the model does not explain any of the variance in the data, and 1 indicates that the model explains all of the variance in the data.\n",
    "\n",
    "For this step we are going to use the test set to evaluate the model. \n",
    "\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 3**: Evaluate Model\n",
    "\n",
    "```python\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    predictions = model(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, targets)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
