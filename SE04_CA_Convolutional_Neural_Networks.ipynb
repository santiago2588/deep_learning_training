{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/se04.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/SE01_CA_Intro_to_pytorch.ipynb)\n",
    "# Workshop Instructions\n",
    "***\n",
    "- <img src=\"figs/icons/write.svg\" width=\"20\"/> Follow along by typing the code yourself - this helps with learning!\n",
    "- <img src=\"figs/icons/code.svg\" width=\"20\"/> Code cells marked as \"Exercise\" are for you to complete\n",
    "- <img src=\"figs/icons/reminder.svg\" width=\"20\"/> Look for hints if you get stuck\n",
    "- <img src=\"figs/icons/success.svg\" width=\"20\" /> Compare your solution with the provided answers\n",
    "- <img src=\"figs/icons/list.svg\" width=\"20\" /> Don't worry if you make mistakes - debugging is part of learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CLDiego/uom_fse_dl_workshop.git\n",
    "%cd UoM_fse_dl_workshop\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_path = Path.cwd()\n",
    "if str(repo_path) not in sys.path:\n",
    "    sys.path.append(str(repo_path))\n",
    "\n",
    "import utils \n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available. Please ensure you've enabled GPU in Runtime > Change runtime type\")\n",
    "\n",
    "checker = utils.core.ExerciseChecker(\"SE04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolutional Neural Networks (CNNs)\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\"/> **Definition**: Convolutional Neural Networks (CNNs) are a specialized type of neural network designed for processing structured grid-like data, such as images, by using mathematical operations called convolutions.\n",
    "\n",
    "CNNs have revolutionized computer vision tasks and are the foundation of many modern systems for image recognition, object detection, segmentation, and more. Their architecture is inspired by the organization of the visual cortex in animals, where individual neurons respond to stimuli in restricted regions called receptive fields.\n",
    "\n",
    "## 1.1 Why Standard Neural Networks Struggle with Images\n",
    "***\n",
    "Images present unique challenges that make standard fully-connected neural networks inefficient:\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| **Spatial Relationships** | Standard networks don't account for spatial relationships between pixels |\n",
    "| **Parameter Explosion** | A 224Ã—224Ã—3 image would require over 150,000 weights per neuron |\n",
    "| **Translation Invariance** | Objects can appear anywhere in an image but have the same meaning |\n",
    "| **Feature Hierarchy** | Images contain low-level features (edges, textures) that compose into higher-level features |\n",
    "***\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/image_rgb.png\" alt=\"Image multi-dimensionality\" align=\"center\" style=\"width: 60%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "CNNs address these challenges through specialized architecture components that we'll explore in this workshop.\n",
    "\n",
    "Let's begin by understanding the core operation that gives CNNs their name: convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Convolution Operation\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\"/> **Definition**: A convolution in the context of CNNs is a mathematical operation that combines two functions by multiplying them and integrating over their overlapping regions.\n",
    "\n",
    "In simple terms, convolution involves sliding a small window (called a filter or kernel) over an image and performing an element-wise multiplication between the filter and the pixel values, then summing the results to produce a single output value for each position.\n",
    "\n",
    "## 2.1 How Convolution Works\n",
    "***\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Position the filter at the top-left corner of the image |\n",
    "| 2 | Perform element-wise multiplication between the filter and the corresponding image pixels |\n",
    "| 3 | Sum all the resulting values to get a single output value |\n",
    "| 4 | Move the filter to the next position (typically one pixel to the right) |\n",
    "| 5 | Repeat steps 2-4 until the entire image has been covered |\n",
    "***\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/convolution_hyperparameters.gif\" alt=\"CNN Structure\" align=\"center\" style=\"width: 50%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "This process creates what's called a feature map, which highlights specific patterns or features in the image that match the filter pattern.\n",
    "\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\"/> **Note**: In deep learning libraries, what's actually implemented is technically cross-correlation rather than convolution (the filter is not flipped). However, since the filters are learned during training, this distinction doesn't matter in practice.\n",
    "\n",
    "Let's first load an example image to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_image = Path.cwd() / \"figs\" / \"ascent.jpg\"\n",
    "asc_image = Image.open(asc_image).resize((256, 256))\n",
    "asc_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Key Parameters in Convolution\n",
    "***\n",
    "The convolution operation is governed by several key parameters that affect the output dimensions and characteristics of the feature map. PyTorch provides a convenient way to implement convolutional layers using the `torch.nn.Conv2d` class. The key parameters include:\n",
    "\n",
    "| Parameter | Description | Effect on Output Dimensions |\n",
    "|-----------|-------------|---------------------------|\n",
    "| **Kernel Size** | The dimensions of the filter (e.g., 3Ã—3, 5Ã—5) | Larger kernels reduce output size more |\n",
    "| **Stride** | How many pixels the filter shifts at each step | Larger strides reduce output dimensions |\n",
    "| **Padding** | Adding extra pixels around the border | Can preserve input dimensions |\n",
    "| **Dilation** | Spacing between kernel elements | Increases receptive field without increasing parameters |\n",
    "\n",
    "Understanding how these parameters affect the output dimensions is crucial for designing effective CNN architectures. The formula for calculating the output dimensions of a convolutional layer is:\n",
    "\n",
    "$$\\text{Output Size} = \\left\\lfloor\\frac{\\text{Input Size} - \\text{Kernel Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1\\right\\rfloor$$\n",
    "\n",
    "where $\\lfloor \\cdot \\rfloor$ represents the floor operation (rounding down to the nearest integer).\n",
    "\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\"/> **Note**: This formula assumes that both the input and kernel are square, but it can be applied separately to height and width for rectangular inputs and kernels.\n",
    "\n",
    "Let's implement a function to calculate the output size of a convolutional layer for various parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Calculating Convolutional Output Dimensions ðŸŽ¯\n",
    "# Implement a function to calculate the output dimensions after applying convolution\n",
    "# with different kernel sizes, strides, and padding values.\n",
    "\n",
    "def calculate_output_size(input_height:int, input_width:int, \n",
    "                          kernel_size:int, stride:int=1, padding:int=0) -> tuple:\n",
    "    \"\"\"Calculate the output dimensions after applying convolution.\n",
    "    \n",
    "    Args:\n",
    "        input_height (int): Height of the input feature map\n",
    "        input_width (int): Width of the input feature map\n",
    "        kernel_size (int): Size of the square kernel\n",
    "        stride (int, optional): Convolution stride. Defaults to 1.\n",
    "        padding (int, optional): Padding size. Defaults to 0.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (output_height, output_width)\n",
    "    \"\"\"\n",
    "    # Your code here: Implement the formula for calculating output dimensions\n",
    "    output_height = # Your code here\n",
    "    output_width = # Your code here\n",
    "    \n",
    "    return output_height, output_width\n",
    "\n",
    "# Test the function with different parameters\n",
    "# Case 1: Standard convolution with a 3x3 kernel, stride=1, no padding\n",
    "input1 = (28, 28)  # e.g., MNIST image size\n",
    "output1 = # Your code here\n",
    "\n",
    "# Case 2: Convolution with padding=1 to preserve dimensions\n",
    "input2 = (224, 224)  # e.g., Standard ImageNet size\n",
    "output2 = # Your code here\n",
    "\n",
    "# Case 3: Convolution with stride=2 for downsampling\n",
    "input3 = (128, 128)\n",
    "output3 = # Your code here\n",
    "\n",
    "# Case 4: Custom parameters\n",
    "input4 = (64, 64)\n",
    "output4 = # Your code here\n",
    "\n",
    "print(f\"Case 1: {input1} â†’ {output1} (3x3 kernel, stride=1, no padding)\")\n",
    "print(f\"Case 2: {input2} â†’ {output2} (3x3 kernel, stride=1, padding=1)\")\n",
    "print(f\"Case 3: {input3} â†’ {output3} (5x5 kernel, stride=2, padding=2)\")\n",
    "print(f\"Case 4: {input4} â†’ {output4} (7x7 kernel, stride=2, padding=3)\")\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'output1': output1[0],\n",
    "    'output2': output2[0],\n",
    "    'output3': output3[0],\n",
    "    'output4': output4[0]\n",
    "}\n",
    "checker.check_exercise(1, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 What is a Filter?\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\"/> **Definition**: A filter (or kernel) is a small matrix used in convolutional operations to extract features from an image. It slides over the image, performing element-wise multiplication and summing the results to produce a single output value.\n",
    "\n",
    "Filters allow CNNs to learn and detect specific patterns, such as edges, textures, and shapes, by adjusting their weights during training. The concept of filters is central to computer vision tasks, and there are existing filters for common tasks, such as edge detection and blurring. Let's explore some of these filters and their effects on images.\n",
    "\n",
    "We are going to try the following filters:\n",
    "\n",
    "| Filter | Kernel | Description |\n",
    "|--------|--------|-------------|\n",
    "| **Edge Detection** | $$\\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1 \\end{bmatrix}$$ | Detects vertical edges |\n",
    "| **Sharpening** | $$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}$$ | Enhances edges and details |\n",
    "| **Embossing** | $$\\begin{bmatrix} -2 & -1 & 0 \\\\ -1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$$ | Creates a 3D effect |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Designing Convolutional Filters with PyTorch ðŸŽ¯\n",
    "# In this exercise, you will implement common filters used in image processing using PyTorch\n",
    "\n",
    "def apply_filter_pytorch(image, kernel):\n",
    "    \"\"\"Apply a convolutional filter to an image using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image (grayscale or RGB)\n",
    "        kernel (numpy.ndarray): Convolutional kernel/filter\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Filtered image\n",
    "    \"\"\"\n",
    "    # Make a copy of the image to avoid modifying the original\n",
    "    image_copy = image.copy().astype(np.float32)\n",
    "    \n",
    "    # For RGB: rearrange to PyTorch format (B, C, H, W)\n",
    "    image_tensor = # Your code here\n",
    "    channels = # Your code here\n",
    "    \n",
    "    # Convert kernel to PyTorch tensor\n",
    "    kernel_tensor = # Your code here\n",
    "    \n",
    "    # Create a convolutional layer with our kernel as weights\n",
    "    # Use groups=channels to apply the same kernel to each channel independently\n",
    "    conv_layer = torch.nn.Conv2d(in_channels=channels, \n",
    "                                 out_channels=channels,\n",
    "                                 kernel_size=kernel.shape[0],\n",
    "                                 bias=False,\n",
    "                                 padding=kernel.shape[0]//2, \n",
    "                                 groups=channels)\n",
    "    \n",
    "    # Set the weights to our kernel\n",
    "    with torch.no_grad():\n",
    "        for i in range(channels):\n",
    "            # Yout code here\n",
    "    \n",
    "    # Apply convolution\n",
    "    with torch.no_grad():\n",
    "        filtered = # Your code here\n",
    "    \n",
    "    # Convert back to numpy array in correct format\n",
    "    filtered_image = # Your code here\n",
    "    \n",
    "    # Clip values to be in valid range for images (0-255)\n",
    "    filtered_image = # Your code here\n",
    "    return filtered_image\n",
    "\n",
    "# Design several common convolutional filters\n",
    "\n",
    "# 1. Edge detection filter (should highlight boundaries between different regions)\n",
    "edge_detection_kernel = np.array([\n",
    "    [1, 0, -1],\n",
    "    [0, 0, 0],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "# 2. Sharpening filter (should enhance details by increasing contrast)\n",
    "sharpen_kernel = np.array([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n",
    "\n",
    "# 3. Embossing filter (should give a 3D effect by highlighting edges with shadows)\n",
    "emboss_kernel = np.array([\n",
    "    [-2, -1, 0],\n",
    "    [-1, 1, 1],\n",
    "    [0, 1, 2]\n",
    "])\n",
    "\n",
    "# Load a test image - use the ascent image we loaded earlier\n",
    "test_image = np.array(asc_image)\n",
    "\n",
    "# Apply the filters to the test image using PyTorch\n",
    "edge_detect_image = apply_filter_pytorch(test_image, edge_detection_kernel)\n",
    "sharpened_image = apply_filter_pytorch(test_image, sharpen_kernel)\n",
    "embossed_image = apply_filter_pytorch(test_image, emboss_kernel)\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "\n",
    "axes[0, 0].imshow(test_image)\n",
    "axes[0, 0].set_title(\"Original Image\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(edge_detect_image)\n",
    "axes[0, 1].set_title(\"Edge Detection\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(sharpened_image)\n",
    "axes[1, 0].set_title(\"Sharpening\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(embossed_image)\n",
    "axes[1, 1].set_title(\"Embossing\")\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'edge_detection_kernel': edge_detection_kernel,\n",
    "    'sharpen_kernel': sharpen_kernel, \n",
    "    'emboss_kernel': emboss_kernel\n",
    "}\n",
    "checker.check_exercise(2, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we used predefined filters, but in practice, the filters are learned during training. The network learns to adjust the filter weights to detect relevant features for the specific task at hand.\n",
    "\n",
    "Let's see how the output of a simple convolution operation looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = torch.nn.Conv2d(\n",
    "    in_channels=3, \n",
    "    out_channels=3, \n",
    "    kernel_size=3, \n",
    "    stride=1, \n",
    "    padding=12, \n",
    ")\n",
    "\n",
    "torch.nn.init.xavier_uniform_(conv2d.weight)\n",
    "\n",
    "# Change the shape of the image to (C, H, W)\n",
    "torch_asc = torch.from_numpy(np.array(asc_image)).permute(2,0,1)\n",
    "torch_asc = torch_asc.unsqueeze(0).float() # Add batch dimension\n",
    "\n",
    "conv2d.eval()\n",
    "filtered_asc = conv2d(torch_asc)\n",
    "# Reverse the transformation to get back to (H, W, C)\n",
    "filtered_asc = filtered_asc.squeeze(0).detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Make sure the values are in the range [0, 255]\n",
    "# and convert to uint8 for PIL\n",
    "filtered_asc = np.clip(filtered_asc, 0, 255).astype(np.uint8) \n",
    "filtered_asc_img = Image.fromarray(filtered_asc)\n",
    "filtered_asc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing image data\n",
    "***\n",
    "Since images can be seen as 3D tensors, we need to convert them into a format suitable for processing. In PyTorch, images are typically represented as 4D tensors with the shape `(batch_size, channels, height, width)`. For a single image, the shape would be `(1, 3, height, width)`.\n",
    "\n",
    "To prepare the image data, we will use the `torchvision` library, which provides convenient functions for loading and transforming images. \n",
    "\n",
    "## 3.1 Torchvision transforms\n",
    "***\n",
    "Python uses `PIL` (Python Imaging Library) to handle images, and while `PIL` is great for basic image manipulation, it can be slow for large datasets. To speed up the process, we can use `torchvision.transforms`, which provides a set of common image transformations that can be applied to images in a more efficient way.\n",
    "\n",
    "| Transform | PyTorch Function | Description |\n",
    "|-----------|------------------|-------------|\n",
    "| **Resize** | `transforms.Resize(size)` | Resizes the image to the specified size |\n",
    "| **CenterCrop** | `transforms.CenterCrop(size)` | Crops the image at the center to the specified size |\n",
    "| **RandomCrop** | `transforms.RandomCrop(size)` | Crops the image randomly to the specified size |\n",
    "| **RandomHorizontalFlip** | `transforms.RandomHorizontalFlip(p)` | Flips the image horizontally with probability `p` |\n",
    "| **RandomRotation** | `transforms.RandomRotation(degrees)` | Rotates the image randomly within the specified degrees |\n",
    "| **Normalize** | `transforms.Normalize(mean, std)` | Normalizes the image tensor with the specified mean and standard deviation |\n",
    "| **ColorJitter** | `transforms.ColorJitter(brightness, contrast, saturation, hue)` | Randomly changes the brightness, contrast, saturation, and hue of the image |\n",
    "| **ToTensor** | `transforms.ToTensor()` | Converts the image to a PyTorch tensor |\n",
    "\n",
    "These transformations can be combined to create a preprocessing pipeline that prepares the images for training. The `transforms.Compose` function allows us to chain multiple transformations together.\n",
    "\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\"/> **Notes**:\n",
    "> - Resizing is important because CNNs require fixed-size inputs.\n",
    "> - The `ToTensor` transformation converts the image to a PyTorch tensor, and it also scales the pixel values to the range [0, 1]. \n",
    "> - Normalization is a common practice in deep learning to ensure that the input data has a mean of 0 and a standard deviation of 1. This helps the model converge faster during training.\n",
    "***\n",
    "\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: Composing transformations\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "ts = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implementing Image Transformations ðŸŽ¯\n",
    "# In this exercise, you will implement and visualize various image transformations\n",
    "# commonly used in computer vision tasks\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def apply_transformations(image_path):\n",
    "    \"\"\"Apply and visualize various image transformations.\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the input image\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of transformed images\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = Image.open(image_path) if isinstance(\n",
    "        image_path, (str, Path)) else image_path\n",
    "\n",
    "    # Define transformations\n",
    "    # 1. Basic resize to 128x128 \n",
    "    resize_transform = transforms.Compose([\n",
    "        # Your code here\n",
    "        # Your code here\n",
    "    ])\n",
    "\n",
    "    # 2. Center crop transformation\n",
    "    center_crop_transform = transforms.Compose([\n",
    "        # Your code here: Resize the smaller edge to 150 pixels\n",
    "        # Your code here: Crop a 100x100 square from the center\n",
    "        # Your code here\n",
    "    ])\n",
    "\n",
    "    # 3. Random crop transformation\n",
    "    random_crop_transform = transforms.Compose([\n",
    "        transforms.Resize(150),\n",
    "        # Your code here: random crop of size 100x100\n",
    "        # Your code here\n",
    "    ])\n",
    "\n",
    "    # 4. Random horizontal flip transformation\n",
    "    hflip_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        # Your code here: 100% flip probability\n",
    "        # Your code here\n",
    "    ])\n",
    "\n",
    "    # 5. Random rotation transformation\n",
    "    rotate_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        # Your code here: random rotation\n",
    "        # Your code here\n",
    "\n",
    "    ])\n",
    "\n",
    "    # 6. Color jitter transformation\n",
    "    color_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        # Your code here: color jitter with brightness, contrast, saturation and hue\n",
    "        # Your code here\n",
    "    ])\n",
    "\n",
    "    # 7. Combined transformations (practical data augmentation)\n",
    "    combined_transform = transforms.Compose([\n",
    "        # Your code here: random resized crop of size 128x128\n",
    "        # Your code here: random horizontal flip \n",
    "        # Your code here: random rotation of 15 degrees\n",
    "        # Your code here: color jitter with brightness and contrast\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 8. Normalization transformation\n",
    "    norm_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        # Your code here: ImageNet normalization with mean and std [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]\n",
    "        # Your code here\n",
    "    ])\n",
    "\n",
    "    transforms_list = {\n",
    "        'Original': transforms.ToTensor(),\n",
    "        'Resized': resize_transform,\n",
    "        'Center Crop': center_crop_transform,\n",
    "        'Random Crop': random_crop_transform,\n",
    "        'Horizontal Flip': hflip_transform,\n",
    "        'Random Rotation': rotate_transform,\n",
    "        'Color Jitter': color_transform,\n",
    "        'Combined': combined_transform,\n",
    "        'Normalized': norm_transform\n",
    "    }\n",
    "\n",
    "    # Apply transformations\n",
    "    transforms_dict = {\n",
    "        'Original': transforms.ToTensor()(img),\n",
    "        'Resized': resize_transform(img),\n",
    "        'Center Crop': center_crop_transform(img),\n",
    "        'Random Crop': random_crop_transform(img),\n",
    "        'Horizontal Flip': hflip_transform(img),\n",
    "        'Random Rotation': rotate_transform(img),\n",
    "        'Color Jitter': color_transform(img),\n",
    "        'Combined': combined_transform(img),\n",
    "        'Normalized': norm_transform(img)\n",
    "    }\n",
    "\n",
    "    return transforms_dict, transforms_list\n",
    "\n",
    "\n",
    "# Apply transformations to an image and visualize the results\n",
    "# Use the ascent image we loaded earlier as a test image\n",
    "ts_dict, ts_list = apply_transformations(asc_image)\n",
    "utils.plotting.se04_visualize_transformations(ts_dict)\n",
    "\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'resize_transform': ts_list['Resized'],\n",
    "    'center_crop_transform': ts_list['Center Crop'],\n",
    "    'random_crop_transform': ts_list['Random Crop'],\n",
    "    'hflip_transform': ts_list['Horizontal Flip'],\n",
    "    'rotate_transform': ts_list['Random Rotation'],\n",
    "    'color_transform': ts_list['Color Jitter'],\n",
    "    'norm_transform': ts_list['Normalized'],\n",
    "}\n",
    "checker.check_exercise(3, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Historical Crack Dataset\n",
    "***\n",
    "In this session, we are going to be working with the *Historical-crack18-19* dataset. The dataset contains annotated images for non-invasive surface crack detection in historical buildings. The goal is to train a model that can accurately identify cracks in these images. The current manual visual inspection of built environments is time-consuming, labor-intensive, prone to errors, costly, and lacks scalability. Therefore, the dataset is designed to facilitate the development of deep learning models for automatic crack detection.\n",
    "\n",
    "The dataset contains:\n",
    "\n",
    "| Attribute | Number of Images | \n",
    "| ----------|------------------|\n",
    "| **Crack** | 757 |\n",
    "| **No crack** | 3,139 |\n",
    "\n",
    "As we can see, the dataset is highly imbalanced, with a significant number of images without cracks. This imbalance can affect the performance of the model, as it may learn to predict the majority class (no crack) more often than the minority class (crack). In a first instance, we are going to take a subset of the dataset to balance the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd() / \"datasets\"\n",
    "dataset_path = utils.data.download_dataset(\"historical cracks\",\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True,\n",
    "                                   remove_compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_crack = dataset_path / \"crack\"\n",
    "img_no_crack = dataset_path / \"non-crack\"\n",
    "\n",
    "# Create a new folder for the balanced dataset\n",
    "balanced_data_path = dataset_path / \"balanced\"\n",
    "balanced_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create train, test, and validation folders\n",
    "train_folder = balanced_data_path / \"train\"\n",
    "train_folder.mkdir(parents=True, exist_ok=True)\n",
    "test_folder = balanced_data_path / \"test\"\n",
    "test_folder.mkdir(parents=True, exist_ok=True)\n",
    "val_folder = balanced_data_path / \"val\"\n",
    "val_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for folder in [train_folder, test_folder, val_folder]:\n",
    "    (folder / \"crack\").mkdir(parents=True, exist_ok=True)\n",
    "    (folder / \"no_crack\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "crack_images = list(img_crack.glob(\"*.jpg\"))\n",
    "no_crack_images = random.sample(list(img_no_crack.glob(\"*.jpg\")), len(crack_images))\n",
    "\n",
    "# Shuffle the images\n",
    "random.shuffle(crack_images)\n",
    "random.shuffle(no_crack_images)\n",
    "\n",
    "# Split the images into train, test, and validation sets\n",
    "train_ix = int(0.8 * len(crack_images))\n",
    "val_ix = int(0.9 * len(crack_images))\n",
    "test_ix = len(crack_images)\n",
    "\n",
    "train_crack_images = crack_images[:train_ix]\n",
    "val_crack_images = crack_images[train_ix:val_ix]\n",
    "test_crack_images = crack_images[val_ix:test_ix]\n",
    "\n",
    "train_no_crack_images = no_crack_images[:train_ix]\n",
    "val_no_crack_images = no_crack_images[train_ix:val_ix]\n",
    "test_no_crack_images = no_crack_images[val_ix:test_ix]\n",
    "\n",
    "# Copy the images to the new folders\n",
    "for img in train_crack_images:\n",
    "    shutil.copy(img, train_folder / \"crack\")\n",
    "for img in train_no_crack_images:\n",
    "    shutil.copy(img, train_folder / \"no_crack\")\n",
    "for img in val_crack_images:\n",
    "    shutil.copy(img, val_folder / \"crack\")\n",
    "for img in val_no_crack_images:\n",
    "    shutil.copy(img, val_folder / \"no_crack\")\n",
    "for img in test_crack_images:\n",
    "    shutil.copy(img, test_folder / \"crack\")\n",
    "for img in test_no_crack_images:\n",
    "    shutil.copy(img, test_folder / \"no_crack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 5 images\n",
    "random_images = random.sample(crack_images, 5)\n",
    "\n",
    "# Display the images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for ax, img_path in zip(axes, random_images):\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(img_path.stem)\n",
    "plt.tight_layout()\n",
    "\n",
    "print (f\"Images size: {img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 PyTorch ImageFolder\n",
    "***\n",
    "While we can load images using `PIL`, PyTorch provides a more efficient way to handle large datasets through the `torchvision.datasets` module. This module contains the `ImageFolder` class, which allows us to load images from a directory structure where each subdirectory represents a class. The `ImageFolder` class automatically assigns labels based on the subdirectory names.\n",
    "\n",
    "The `ImageFolder` class requires a root directory containing subdirectories for each class. The directory structure should look like this:\n",
    "\n",
    "```bash\n",
    "dataset/\n",
    "    â”œâ”€â”€ class-1/\n",
    "    â”‚   â”œâ”€â”€ image1.jpg\n",
    "    â”‚   â”œâ”€â”€ image2.jpg\n",
    "    â”‚   â””â”€â”€ ...\n",
    "    â””â”€â”€ class-2/\n",
    "        â”œâ”€â”€ image1.jpg\n",
    "        â”œâ”€â”€ image2.jpg\n",
    "        â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "The function automatically assigns labels to the images based on the subdirectory names. Moreover, it can also apply transformations to the images using the `transform` parameter.\n",
    "\n",
    "The key parameters of the `ImageFolder` class are:\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| **root** | The root directory containing the dataset |\n",
    "| **transform** | A function/transform to apply to the images |\n",
    "| **target_transform** | A function/transform to apply to the target (label) |\n",
    "| **loader** | A function to load the images (default is `PIL.Image.open`) |\n",
    "| **is_valid_file** | A function to check if a file is valid (default is `None`) |\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\"/> **Snippet 2**: Using ImageFolder\n",
    "```python\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset = ImageFolder(root='path/to/dataset', transform=ts)\n",
    "# Accessing the first image and its label\n",
    "image, label = dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Data Augmentation and Loading with PyTorch ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Data augmentation techniques\n",
    "# 2. Data loading with ImageFolder\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "ts_train = transforms.Compose([\n",
    "    # Your code here: Add transforms for training data\n",
    "    # Resize to 64x64, add random horizontal flip\n",
    "    # Add random rotation, add color jitter\n",
    "    # Convert to tensor\n",
    "    # Your code here\n",
    "])\n",
    "\n",
    "ts_test_val = transforms.Compose([\n",
    "    # Your code here: Add transforms for test/val data\n",
    "    # We only need to resize and convert to tensor for test/val\n",
    "    # Your code here\n",
    "])\n",
    "\n",
    "# Create datasets using ImageFolder\n",
    "train_data = # Your code here\n",
    "test_data = # Your code here\n",
    "val_data = # Your code here\n",
    "\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'train_transforms': ts_train,\n",
    "    'test_val_transforms': ts_test_val,\n",
    "    'train_data': train_data,\n",
    "    'test_data': test_data,\n",
    "    'val_data': val_data\n",
    "}\n",
    "checker.check_exercise(4, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 PyTorch DataLoaders\n",
    "***\n",
    "As we discussed in the previous session, when training a model we need to load the data in batches. PyTorch provides the `DataLoader` class to handle this efficiently. The `DataLoader` class takes a dataset and provides an iterable over the dataset, allowing us to load data in batches.\n",
    "\n",
    "The model expects our image data to be formatted as a 4D tensor with the shape `(batch_size, channels, height, width)`.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/image_batches.png\" alt=\"DataLoader\" align=\"center\" style=\"width: 50%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "The `DataLoader` class provides several key parameters to customize the data loading process:\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| **dataset** | The dataset to load data from (e.g., `ImageFolder`) |\n",
    "| **batch_size** | The number of samples per batch |\n",
    "| **shuffle** | Whether to shuffle the data at every epoch |\n",
    "| **num_workers** | The number of subprocesses to use for data loading |\n",
    "| **pin_memory** | Whether to pin memory for faster data transfer to GPU |\n",
    "| **drop_last** | Whether to drop the last incomplete batch |\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\"/> **Snippet 3**: Creating a DataLoader\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# Iterate through the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch shape: {images.shape}, Labels: {labels}\")\n",
    "    break  # Just to show the first batch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: DataLoader ðŸŽ¯\n",
    "# Create DataLoaders for train, test, and validation data\n",
    "\n",
    "# Your code here: create train_dl with batch_size=32 and shuffle=True\n",
    "train_dl = # Your code here\n",
    "# Your code here: create test_dl with batch_size=32 and shuffle=False\n",
    "test_dl = # Your code here\n",
    "# Your code here: create val_dl with batch_size=32 and shuffle=False\n",
    "val_dl = # Your code here\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'train_dataloader': train_dl,\n",
    "    'test_dataloader': test_dl,\n",
    "    'val_dataloader': val_dl,\n",
    "    'batch_size': train_dl.batch_size\n",
    "}\n",
    "checker.check_exercise(5, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementing CNNs\n",
    "***\n",
    "The architecture of a CNN is not that different from a standard neural network. The main difference is that CNNs use convolutional layers instead of fully connected layers. This means that after each convolutional layer, we typically apply a non-linear activation function (like ReLU). \n",
    "\n",
    "The output of the CNN is then passed through one or more fully connected layers to produce the final output. Thus, we need to keep track of the output size after each layer to ensure that the dimensions match up correctly.\n",
    "\n",
    "A diagram of a conventional CNN architecture is shown below.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/cnn.png\" alt=\"CNN Architecture\" align=\"center\" style=\"width: 90%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "We are going to implement a simple CNN architecture for the crack detection task. The architecture consists of the following layers:\n",
    "\n",
    "| Type | Layer | Input Size | Output Size | Activation Function |\n",
    "|-------|-------|------------|-------------|---------------------|\n",
    "| Convolution | `Conv2d` | `(3, 64, 64)` | `(16, 64, 64)` | ReLU |\n",
    "| Fully Connected | `Linear` | `(16 * 64 * 64)` | `16` | ReLU |\n",
    "| Fully Connected | `Linear` | `16` | `2` | None |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Implementing a Simple CNN Model ðŸŽ¯\n",
    "class simpleCNN(torch.nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(simpleCNN, self).__init__()\n",
    "        # Your code here: create a convolutional layer with 3 input channels, 16 output channels\n",
    "        # kernel_size=3, stride=1, padding=1\n",
    "        self.conv1 = # Your code here\n",
    "        # Your code here: create a fully connected layer (Linear) with input 16*64*64 and output 16\n",
    "        self.fc1 = # Your code here\n",
    "        # Your code here: create a fully connected layer (Linear) with input 16 and output n_classes\n",
    "        self.fc2 = # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here: Apply ReLU activation to conv1 output\n",
    "        x = # Your code here\n",
    "        # Your code here: Flatten the tensor\n",
    "        x = # Your code here\n",
    "        # Your code here: Apply ReLU activation to fc1 output\n",
    "        x = # Your code here\n",
    "        # Your code here: Feed through fc2\n",
    "        x = # Your code here\n",
    "        return x\n",
    "\n",
    "model_v1 = simpleCNN(len(train_data.classes))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model_v1.parameters(), lr=3e-3)\n",
    "num_epochs = 10\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'model_architecture': model_v1,\n",
    "    'conv_layer': model_v1.conv1,\n",
    "    'linear_layers': {'count': 2, 'output_features': model_v1.fc2.out_features},\n",
    "    'activation': {'function': 'ReLU', 'count': 2},\n",
    "}\n",
    "checker.check_exercise(6, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = utils.ml.train_model(model_v1,\n",
    "                                criterion,\n",
    "                                optimiser,\n",
    "                                train_loader=train_dl,\n",
    "                                val_loader=val_dl,\n",
    "                                num_epochs=num_epochs,\n",
    "                                plot_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting predictions\n",
    "***\n",
    "The output of the last layer gives us the predicted class probabilities for the two classes: crack and no crack. Therefore, in order for us to get the predicted class, we need to apply a softmax function to the output of the last layer. However, PyTorch `CrossEntropyLoss` combines the softmax and the negative log-likelihood loss in a single function, so we don't need to apply softmax explicitly. The `CrossEntropyLoss` function expects the raw logits (the output of the last layer) as input, and it will apply softmax internally.\n",
    "\n",
    "To predict the class, we can use the `torch.argmax` function to get the index of the maximum value in the output tensor. This index corresponds to the predicted class.\n",
    "\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\"/> **Snippet 4**: Obtaining the predicted class\n",
    "```python\n",
    "model_v1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dl:\n",
    "        outputs = model_v1(images.to(device))\n",
    "        print(outputs[:-1])\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        print(predicted)\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Evaluating the Model ðŸŽ¯\n",
    "# Compute accuracy and classification report\n",
    "\n",
    "# Your code here: Use the utils.ml.compute_accuracy function to compute accuracy on the test set\n",
    "acc = # Your code here\n",
    "print(f\"Test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Your code here: Use the utils.ml.compute_classification_report function to compute the classification report\n",
    "cls_report = # Your code here\n",
    "print('-' * 60)\n",
    "print(f\"Classification Report:\\n{cls_report}\")\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'test_accuracy': acc,\n",
    "    'classification_report': cls_report\n",
    "}\n",
    "checker.check_exercise(7, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model predictions\n",
    "utils.plotting.show_model_predictions(model_v1, test_dl, class_names=train_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Recreating CNN architectures\n",
    "***\n",
    "There are many different CNN architectures that have been proposed in the literature, each with its own strengths and weaknesses. Some of the most popular architectures include:\n",
    "\n",
    "| Architecture | Description | Key Features |\n",
    "|--------------|-------------|--------------|\n",
    "| **LeNet** | One of the first CNN architectures, designed for handwritten digit recognition | 5 layers, small kernel sizes |\n",
    "| **AlexNet** | A deeper architecture that won the ImageNet competition in 2012 | 8 layers, ReLU activation, dropout, data augmentation |\n",
    "| **VGG** | A very deep architecture with small kernel sizes | 16-19 layers, uniform architecture, small kernels |\n",
    "| **ResNet** | Introduced residual connections to allow for very deep networks | 50-152 layers, skip connections, batch normalization |\n",
    "| **Inception** | Introduced the inception module for multi-scale feature extraction | 22-164 layers, parallel convolutions, pooling layers |\n",
    "| **DenseNet** | Introduced dense connections between layers | 121-201 layers, dense connections, feature reuse |\n",
    "\n",
    "These architectures have been shown to perform well on a variety of tasks, and they can be used as a starting point for building custom CNNs. Furthermore, many of these architectures are often visualised as a series of blocks, where each block consists of a convolutional layer followed by an activation function and a pooling layer. We are going to implement a version of the original VGG architecture, which looks like this:\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/vgg.png\" alt=\"VGG Architecture\" align=\"center\" style=\"width: 90%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "## 4.3 Pooling\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\"/> **Definition**: Pooling is a downsampling operation used in CNNs to reduce the spatial dimensions of feature maps while retaining important information. \n",
    "\n",
    "Pooling helps to reduce the number of parameters and computations in the network, making it more efficient and less prone to overfitting. There are several types of pooling operations, but the most common ones are:\n",
    "\n",
    "| Pooling Type | PyTorch Function | Description |\n",
    "|--------------|------------------|-------------|\n",
    "| **Max Pooling** | `torch.nn.MaxPool2d(kernel_size, stride)` | Takes the maximum value in each region defined by the kernel size |\n",
    "| **Average Pooling** | `torch.nn.AvgPool2d(kernel_size, stride)` | Takes the average value in each region defined by the kernel size |\n",
    "| **Global Average Pooling** | `torch.nn.AdaptiveAvgPool2d(output_size)` | Reduces each feature map to a single value by averaging over the entire map |\n",
    "| **Global Max Pooling** | `torch.nn.AdaptiveMaxPool2d(output_size)` | Reduces each feature map to a single value by taking the maximum over the entire map |\n",
    "\n",
    "The pooling operation can be visualised like this:\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/pooling.png\" alt=\"Pooling Operation\" align=\"center\" style=\"width: 40%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "Most commonly, we use max pooling, as it helps to retain the most important features while discarding less relevant information. The pooling operation is typically applied after a convolutional layer and an activation function. \n",
    "\n",
    "## 4.4 Regularisation\n",
    "***\n",
    "As briefly mentioned in the previous session, regularisation is a technique used to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well, including noise and outliers, leading to poor generalisation on unseen data. In CNNs, regularisation techniques are crucial due to the large number of parameters and the complexity of the models. Some common regularisation techniques used in CNNs include:\n",
    "\n",
    "| Regularisation Technique | Pytorch Function | Description |\n",
    "|--------------------------|------------------|-------------|\n",
    "| **Dropout** | `torch.nn.Dropout(p)` | Randomly sets a fraction of input units to 0 at each update during training time, which helps prevent overfitting |\n",
    "| **L2 Regularisation** | `torch.nn.functional.mse_loss()` | Adds a penalty on the size of the weights to the loss function. This is also known as weight decay |\n",
    "| **Batch Normalisation** | `torch.nn.BatchNorm2d(num_features)` | Normalises the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This helps to stabilise the learning process and can lead to faster convergence |\n",
    "| **Data Augmentation** | `torchvision.transforms` | Increases the diversity of the training set by applying random transformations to the input data, such as rotation, translation, and scaling. This helps to improve the generalisation of the model |\n",
    "| **Early Stopping** | `torch.nn.utils` | Stops training when the validation loss stops improving, preventing overfitting |\n",
    "| **Weight Decay** | `torch.optim.AdamW` | Adds a penalty on the size of the weights to the loss function, similar to L2 regularisation. This is also known as weight decay |\n",
    "\n",
    "For our tiny VGG architecture, we are going to use dropout and batch normalisation. The dropout layer is applied after the activation function of the fully connected layers, while the batch normalisation layer is applied after the convolutional layers.\n",
    "\n",
    "Our tiny VGG architecture will look like this:\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/tiny_vgg.png\" alt=\"Tiny VGG Architecture\" align=\"center\" style=\"width: 90%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Implementing a More Complex CNN Model ðŸŽ¯\n",
    "class tinyVGG(torch.nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        # Create convolutional layers\n",
    "        # Your code here: conv1 with 3 input channels, 16 output channels\n",
    "        self.conv1 = # Your code here\n",
    "        # Your code here: conv2 with 16 input channels, 32 output channels\n",
    "        self.conv2 = # Your code here\n",
    "        # Your code here: conv3 with 32 input channels, 64 output channels\n",
    "        self.conv3 = # Your code here\n",
    "        \n",
    "        # Add pooling layers\n",
    "        # Your code here: Create max pooling layer with kernel_size=2, stride=2\n",
    "        self.pool = # Your code here\n",
    "        \n",
    "        # Your code here: Create flatten layer\n",
    "        self.flat = # Your code here\n",
    "        # Adjust input size for fully connected layer due to pooling\n",
    "        # Your code here: Create fc1 with input 64*8*8 and output 128\n",
    "        self.fc1 = # Your code here\n",
    "        # Your code here: Create fc2 with input 128 and output n_classes\n",
    "        self.fc2 = # Your code here\n",
    "        \n",
    "        # dropout for regularization\n",
    "        # Your code here: Create dropout1 with p=0.05\n",
    "        self.dropout1 = # Your code here\n",
    "        # Your code here: Create dropout2 with p=0.1\n",
    "        self.dropout2 = # Your code here\n",
    "        \n",
    "        # batch normalization for more stable training\n",
    "        # Your code here: Create batch_norm1 for 16 features\n",
    "        self.batch_norm1 = # Your code here\n",
    "        # Your code here: Create batch_norm2 for 32 features\n",
    "        self.batch_norm2 = # Your code here\n",
    "        # Your code here: Create batch_norm3 for 64 features\n",
    "        self.batch_norm3 = # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here: Apply conv1, batch_norm1, ReLU, and pooling\n",
    "        x = # Your code here\n",
    "        x = # Your code here\n",
    "        # Your code here: Apply conv2, batch_norm2, ReLU, and pooling\n",
    "        x = # Your code here\n",
    "        x = # Your code here\n",
    "        # Your code here: Apply conv3, batch_norm3, ReLU, and pooling\n",
    "        x = # Your code here\n",
    "        x = # Your code here\n",
    "        # Your code here: Flatten the tensor\n",
    "        x = # Your code here\n",
    "        # Your code here: Apply fc1, ReLU, and dropout2\n",
    "        x = # Your code here\n",
    "        # Your code here: Apply fc2\n",
    "        x = # Your code here\n",
    "        return x\n",
    "\n",
    "\n",
    "# âœ… Check your answer\n",
    "model_v2 = tinyVGG(len(train_data.classes))\n",
    "answer = {\n",
    "    'model_architecture': model_v2,\n",
    "    'conv_layers': model_v2,\n",
    "    'pooling_layers': model_v2.pool,\n",
    "    'batch_norm_layers': [model_v2.batch_norm1, model_v2.batch_norm2, model_v2.batch_norm3],\n",
    "    'dropout_layers': [model_v2.dropout1, model_v2.dropout2],\n",
    "    'flatten_operation': model_v2.flat\n",
    "}\n",
    "checker.check_exercise(8, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = tinyVGG(len(train_data.classes))\n",
    "criterion_reg = torch.nn.CrossEntropyLoss()\n",
    "optimiser_reg = torch.optim.Adam(model_v2.parameters(),\n",
    "                                 lr=1e-3,\n",
    "                                 betas=(0.9, 0.999),\n",
    "                                #  weight_decay=1e-5,  # L2 regularization (weight decay)\n",
    "                                 ) \n",
    "num_epochs_reg = 45\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimiser_reg,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = utils.ml.train_model(model_v2,\n",
    "                  criterion_reg,\n",
    "                  optimiser_reg,\n",
    "                  train_loader=train_dl,\n",
    "                  val_loader=val_dl,\n",
    "                  num_epochs=num_epochs_reg,\n",
    "                  early_stopping=True,\n",
    "                  patience=5,\n",
    "                  tolerance=1e-2,\n",
    "                  save_path= Path.cwd() / \"my_models\" / \"se04_model_v2.pt\",\n",
    "                  plot_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_v2.load_state_dict(torch.load(Path.cwd() / \"my_models\" / \"model_v2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: Evaluating the tiny VGGðŸŽ¯\n",
    "# Your code here: Use the utils.ml.compute_accuracy function to compute accuracy on the test set\n",
    "acc = # Your code here\n",
    "print(f\"Test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Your code here: Use the utils.ml.compute_classification_report function to compute the classification report\n",
    "cls_report = # Your code here\n",
    "print('-' * 60)\n",
    "print(f\"Classification Report:\\n{cls_report}\")\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'test_accuracy': acc,\n",
    "    'classification_report': cls_report\n",
    "}\n",
    "checker.check_exercise(9, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plotting.show_model_predictions(model_v2, test_dl, class_names=train_data.classes, num_images=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
